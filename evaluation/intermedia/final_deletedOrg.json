{
    "Yi-1.5-6B_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.6276883634809857,
                "acc_stderr,none": 0.003838645572251585,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.5536663124335813,
                "acc_stderr,none": 0.006701222777331458,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.47619047619047616,
                "acc_stderr,none": 0.04467062628403273
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.7575757575757576,
                "acc_stderr,none": 0.03346409881055953
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.7843137254901961,
                "acc_stderr,none": 0.028867431449849313
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.7763713080168776,
                "acc_stderr,none": 0.027123298205229966
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.8016528925619835,
                "acc_stderr,none": 0.03640118271990947
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.7592592592592593,
                "acc_stderr,none": 0.041331194402438376
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.7239263803680982,
                "acc_stderr,none": 0.035123852837050475
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.6820809248554913,
                "acc_stderr,none": 0.025070713719153186
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.2424581005586592,
                "acc_stderr,none": 0.014333522059217887
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.7266881028938906,
                "acc_stderr,none": 0.02531176597542612
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.6820987654320988,
                "acc_stderr,none": 0.025910063528240893
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.48435462842242505,
                "acc_stderr,none": 0.012763982838120958
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.7953216374269005,
                "acc_stderr,none": 0.030944459778533204
            },
            "mmlu_other": {
                "acc,none": 0.6935951078210493,
                "acc_stderr,none": 0.007994923670823285,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.67,
                "acc_stderr,none": 0.04725815626252609
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.7018867924528301,
                "acc_stderr,none": 0.028152837942493857
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.630057803468208,
                "acc_stderr,none": 0.0368122963339432
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.38,
                "acc_stderr,none": 0.04878317312145632
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.6860986547085202,
                "acc_stderr,none": 0.031146796482972465
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.8058252427184466,
                "acc_stderr,none": 0.03916667762822582
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.8717948717948718,
                "acc_stderr,none": 0.021901905115073336
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.7,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.8071519795657727,
                "acc_stderr,none": 0.014108533515757433
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.6862745098039216,
                "acc_stderr,none": 0.026568921015457155
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.4929078014184397,
                "acc_stderr,none": 0.02982449855912901
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.6470588235294118,
                "acc_stderr,none": 0.02902942281568141
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.5301204819277109,
                "acc_stderr,none": 0.03885425420866766
            },
            "mmlu_social_sciences": {
                "acc,none": 0.7393565160870978,
                "acc_stderr,none": 0.0077665217559597,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.4649122807017544,
                "acc_stderr,none": 0.046920083813689104
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.7777777777777778,
                "acc_stderr,none": 0.02962022787479047
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.8601036269430051,
                "acc_stderr,none": 0.02503387058301517
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.7153846153846154,
                "acc_stderr,none": 0.022878322799706308
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.7941176470588235,
                "acc_stderr,none": 0.026265024608275882
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.8238532110091743,
                "acc_stderr,none": 0.016332882393431378
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.6870229007633588,
                "acc_stderr,none": 0.04066962905677698
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.6683006535947712,
                "acc_stderr,none": 0.01904748523936038
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.6363636363636364,
                "acc_stderr,none": 0.046075820907199756
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.6857142857142857,
                "acc_stderr,none": 0.02971932942241747
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.8109452736318408,
                "acc_stderr,none": 0.027686913588013028
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.85,
                "acc_stderr,none": 0.035887028128263714
            },
            "mmlu_stem": {
                "acc,none": 0.5642245480494766,
                "acc_stderr,none": 0.00853001813349147,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.35,
                "acc_stderr,none": 0.0479372485441102
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.5703703703703704,
                "acc_stderr,none": 0.04276349494376599
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.625,
                "acc_stderr,none": 0.039397364351956274
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.7152777777777778,
                "acc_stderr,none": 0.03773809990686935
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.43,
                "acc_stderr,none": 0.049756985195624284
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.58,
                "acc_stderr,none": 0.049604496374885836
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.46,
                "acc_stderr,none": 0.05009082659620332
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.4215686274509804,
                "acc_stderr,none": 0.049135952012744975
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.79,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.6638297872340425,
                "acc_stderr,none": 0.030881618520676942
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.6137931034482759,
                "acc_stderr,none": 0.04057324734419035
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.4947089947089947,
                "acc_stderr,none": 0.02574986828855657
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.7903225806451613,
                "acc_stderr,none": 0.023157879349083522
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.5763546798029556,
                "acc_stderr,none": 0.034767257476490364
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.75,
                "acc_stderr,none": 0.04351941398892446
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.37407407407407406,
                "acc_stderr,none": 0.029502861128955286
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.4503311258278146,
                "acc_stderr,none": 0.04062290018683775
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.5231481481481481,
                "acc_stderr,none": 0.03406315360711507
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.4375,
                "acc_stderr,none": 0.04708567521880525
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.6276883634809857,
                "acc_stderr,none": 0.003838645572251585,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.5536663124335813,
                "acc_stderr,none": 0.006701222777331458,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.6935951078210493,
                "acc_stderr,none": 0.007994923670823285,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.7393565160870978,
                "acc_stderr,none": 0.0077665217559597,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.5642245480494766,
                "acc_stderr,none": 0.00853001813349147,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=01-ai/Yi-1.5-6B",
            "model_num_parameters": 6061035520,
            "model_dtype": "torch.bfloat16",
            "model_revision": "main",
            "model_sha": "157a3d77ab5a8a8f079510eaa25169b547481f06",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:1",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721449488.626632,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<unk>",
            "0"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "2"
        ],
        "tokenizer_bos_token": [
            "<|startoftext|>",
            "1"
        ],
        "eot_token_id": 2,
        "max_length": 4096
    },
    "Yi-1.5-6B_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.652289820136085,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=01-ai/Yi-1.5-6B",
            "model_num_parameters": 6061035520,
            "model_dtype": "torch.bfloat16",
            "model_revision": "main",
            "model_sha": "157a3d77ab5a8a8f079510eaa25169b547481f06",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:1",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721448908.8148758,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<unk>",
            "0"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "2"
        ],
        "tokenizer_bos_token": [
            "<|startoftext|>",
            "1"
        ],
        "eot_token_id": 2,
        "max_length": 4096
    },
    "Yi-1.5-9B_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.6839481555333998,
                "acc_stderr,none": 0.0036743158936708847,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.6078639744952179,
                "acc_stderr,none": 0.006543491122731662,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.5079365079365079,
                "acc_stderr,none": 0.044715725362943486
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.806060606060606,
                "acc_stderr,none": 0.03087414513656208
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.8431372549019608,
                "acc_stderr,none": 0.025524722324553332
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.8523206751054853,
                "acc_stderr,none": 0.02309432958259569
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.7933884297520661,
                "acc_stderr,none": 0.03695980128098826
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.7685185185185185,
                "acc_stderr,none": 0.04077494709252627
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.7975460122699386,
                "acc_stderr,none": 0.031570650789119005
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.7601156069364162,
                "acc_stderr,none": 0.022989592543123567
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.3039106145251397,
                "acc_stderr,none": 0.015382845587584505
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.7877813504823151,
                "acc_stderr,none": 0.023222756797435126
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.7777777777777778,
                "acc_stderr,none": 0.023132376234543325
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.5241199478487614,
                "acc_stderr,none": 0.012755368722863945
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.8421052631578947,
                "acc_stderr,none": 0.027966785859160865
            },
            "mmlu_other": {
                "acc,none": 0.7392983585452205,
                "acc_stderr,none": 0.007565189040625942,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.76,
                "acc_stderr,none": 0.04292346959909281
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.7169811320754716,
                "acc_stderr,none": 0.027724236492700918
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.6820809248554913,
                "acc_stderr,none": 0.0355068398916558
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.38,
                "acc_stderr,none": 0.048783173121456316
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.7219730941704036,
                "acc_stderr,none": 0.03006958487449403
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.8058252427184466,
                "acc_stderr,none": 0.03916667762822583
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.9102564102564102,
                "acc_stderr,none": 0.01872430174194166
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.8,
                "acc_stderr,none": 0.040201512610368445
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.8390804597701149,
                "acc_stderr,none": 0.01314022551561173
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.7908496732026143,
                "acc_stderr,none": 0.02328768531233481
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.5460992907801419,
                "acc_stderr,none": 0.02970045324729147
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.7316176470588235,
                "acc_stderr,none": 0.026917481224377232
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.5180722891566265,
                "acc_stderr,none": 0.038899512528272166
            },
            "mmlu_social_sciences": {
                "acc,none": 0.7903802404939877,
                "acc_stderr,none": 0.007187158182904914,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.5526315789473685,
                "acc_stderr,none": 0.046774730044912
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.8686868686868687,
                "acc_stderr,none": 0.024063156416822523
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.9067357512953368,
                "acc_stderr,none": 0.020986854593289708
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.7435897435897436,
                "acc_stderr,none": 0.022139081103971534
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.8697478991596639,
                "acc_stderr,none": 0.021863258494852118
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.8660550458715597,
                "acc_stderr,none": 0.014602811435592635
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.7786259541984732,
                "acc_stderr,none": 0.03641297081313729
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.7238562091503268,
                "acc_stderr,none": 0.018087276935663133
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.6727272727272727,
                "acc_stderr,none": 0.0449429086625209
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.7306122448979592,
                "acc_stderr,none": 0.02840125202902294
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.8109452736318408,
                "acc_stderr,none": 0.02768691358801303
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.92,
                "acc_stderr,none": 0.027265992434429093
            },
            "mmlu_stem": {
                "acc,none": 0.6390738978750397,
                "acc_stderr,none": 0.008225969503914613,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.48,
                "acc_stderr,none": 0.050211673156867795
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.6962962962962963,
                "acc_stderr,none": 0.039725528847851375
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.7828947368421053,
                "acc_stderr,none": 0.03355045304882923
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.7916666666666666,
                "acc_stderr,none": 0.033961162058453336
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.47,
                "acc_stderr,none": 0.05016135580465919
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.58,
                "acc_stderr,none": 0.049604496374885836
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.57,
                "acc_stderr,none": 0.049756985195624284
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.4215686274509804,
                "acc_stderr,none": 0.04913595201274498
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.79,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.7148936170212766,
                "acc_stderr,none": 0.02951319662553935
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.6413793103448275,
                "acc_stderr,none": 0.039966295748767186
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.6322751322751323,
                "acc_stderr,none": 0.024833839825562427
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.8419354838709677,
                "acc_stderr,none": 0.02075283151187526
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.6600985221674877,
                "acc_stderr,none": 0.033327690684107895
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.79,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.4,
                "acc_stderr,none": 0.029869605095316904
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.4768211920529801,
                "acc_stderr,none": 0.04078093859163084
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.6759259259259259,
                "acc_stderr,none": 0.03191923445686186
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.5,
                "acc_stderr,none": 0.04745789978762494
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.6839481555333998,
                "acc_stderr,none": 0.0036743158936708847,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.6078639744952179,
                "acc_stderr,none": 0.006543491122731662,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.7392983585452205,
                "acc_stderr,none": 0.007565189040625942,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.7903802404939877,
                "acc_stderr,none": 0.007187158182904914,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.6390738978750397,
                "acc_stderr,none": 0.008225969503914613,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=01-ai/Yi-1.5-9B",
            "model_num_parameters": 8829407232,
            "model_dtype": "torch.bfloat16",
            "model_revision": "main",
            "model_sha": "80d5471b1eae28beae33e06eadbd4b48e74d4ce1",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:1",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721452659.892403,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<unk>",
            "0"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "2"
        ],
        "tokenizer_bos_token": [
            "<|startoftext|>",
            "1"
        ],
        "eot_token_id": 2,
        "max_length": 4096
    },
    "Yi-1.5-9B_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.7425082163785829,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=01-ai/Yi-1.5-9B",
            "model_num_parameters": 8829407232,
            "model_dtype": "torch.bfloat16",
            "model_revision": "main",
            "model_sha": "80d5471b1eae28beae33e06eadbd4b48e74d4ce1",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:1",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721450195.4435465,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<unk>",
            "0"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "2"
        ],
        "tokenizer_bos_token": [
            "<|startoftext|>",
            "1"
        ],
        "eot_token_id": 2,
        "max_length": 4096
    },
    "bloom-1b1_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.24035037743911125,
                "acc_stderr,none": 0.0036025558917477545,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.24697130712008503,
                "acc_stderr,none": 0.0062882473917036325,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2222222222222222,
                "acc_stderr,none": 0.037184890068181146
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.21818181818181817,
                "acc_stderr,none": 0.03225078108306289
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.24019607843137256,
                "acc_stderr,none": 0.02998373305591361
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.28270042194092826,
                "acc_stderr,none": 0.029312814153955914
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.2396694214876033,
                "acc_stderr,none": 0.03896878985070417
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.23148148148148148,
                "acc_stderr,none": 0.04077494709252627
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.25153374233128833,
                "acc_stderr,none": 0.034089978868575295
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.26878612716763006,
                "acc_stderr,none": 0.02386800326250011
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.24692737430167597,
                "acc_stderr,none": 0.014422292204808838
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.20257234726688103,
                "acc_stderr,none": 0.022827317491059675
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.2191358024691358,
                "acc_stderr,none": 0.023016705640262196
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.2529335071707953,
                "acc_stderr,none": 0.011102268713839989
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.2982456140350877,
                "acc_stderr,none": 0.035087719298245654
            },
            "mmlu_other": {
                "acc,none": 0.2574831026713872,
                "acc_stderr,none": 0.007831139613192565,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.25,
                "acc_stderr,none": 0.04351941398892446
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.2339622641509434,
                "acc_stderr,none": 0.02605529690115292
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.24855491329479767,
                "acc_stderr,none": 0.03295304696818317
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.33,
                "acc_stderr,none": 0.047258156262526045
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.29596412556053814,
                "acc_stderr,none": 0.03063659134869981
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.20388349514563106,
                "acc_stderr,none": 0.039891398595317706
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.31196581196581197,
                "acc_stderr,none": 0.030351527323344937
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.29,
                "acc_stderr,none": 0.045604802157206845
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.24776500638569604,
                "acc_stderr,none": 0.015438083080568966
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.2581699346405229,
                "acc_stderr,none": 0.025058503316958147
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.24468085106382978,
                "acc_stderr,none": 0.025645553622266736
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.1875,
                "acc_stderr,none": 0.023709788253811766
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.3313253012048193,
                "acc_stderr,none": 0.03664314777288085
            },
            "mmlu_social_sciences": {
                "acc,none": 0.22684432889177772,
                "acc_stderr,none": 0.007544546522746733,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.22807017543859648,
                "acc_stderr,none": 0.03947152782669415
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.16666666666666666,
                "acc_stderr,none": 0.026552207828215293
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.21243523316062177,
                "acc_stderr,none": 0.02951928261681726
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.19487179487179487,
                "acc_stderr,none": 0.020083167595181393
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.23109243697478993,
                "acc_stderr,none": 0.027381406927868963
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.21834862385321102,
                "acc_stderr,none": 0.017712600528722727
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.2366412213740458,
                "acc_stderr,none": 0.03727673575596918
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.27124183006535946,
                "acc_stderr,none": 0.01798661530403029
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.24545454545454545,
                "acc_stderr,none": 0.041220665028782834
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.2,
                "acc_stderr,none": 0.025607375986579153
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.23880597014925373,
                "acc_stderr,none": 0.030147775935409227
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.27,
                "acc_stderr,none": 0.044619604333847394
            },
            "mmlu_stem": {
                "acc,none": 0.22676815731049793,
                "acc_stderr,none": 0.007456386876988137,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.27,
                "acc_stderr,none": 0.044619604333847394
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.1925925925925926,
                "acc_stderr,none": 0.03406542058502652
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.24342105263157895,
                "acc_stderr,none": 0.034923496688842384
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2777777777777778,
                "acc_stderr,none": 0.03745554791462457
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.19,
                "acc_stderr,none": 0.03942772444036623
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.26,
                "acc_stderr,none": 0.044084400227680794
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.24,
                "acc_stderr,none": 0.04292346959909282
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.22549019607843138,
                "acc_stderr,none": 0.04158307533083286
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.25,
                "acc_stderr,none": 0.04351941398892446
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.24680851063829787,
                "acc_stderr,none": 0.028185441301234123
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2689655172413793,
                "acc_stderr,none": 0.036951833116502325
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.2275132275132275,
                "acc_stderr,none": 0.021591269407823778
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.19032258064516128,
                "acc_stderr,none": 0.022331707611823074
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.1921182266009852,
                "acc_stderr,none": 0.027719315709614775
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.26,
                "acc_stderr,none": 0.0440844002276808
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.25555555555555554,
                "acc_stderr,none": 0.026593939101844086
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.2185430463576159,
                "acc_stderr,none": 0.033742355504256936
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.1574074074074074,
                "acc_stderr,none": 0.02483717351824239
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.22321428571428573,
                "acc_stderr,none": 0.039523019677025116
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.24035037743911125,
                "acc_stderr,none": 0.0036025558917477545,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.24697130712008503,
                "acc_stderr,none": 0.0062882473917036325,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.2574831026713872,
                "acc_stderr,none": 0.007831139613192565,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.22684432889177772,
                "acc_stderr,none": 0.007544546522746733,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.22676815731049793,
                "acc_stderr,none": 0.007456386876988137,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=bigscience/bloom-1b1",
            "model_num_parameters": 1065314304,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "eb3dd7399312f5f94fd13f41d2f318117d3eb1e4",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:1",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721454050.0444272,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<pad>",
            "3"
        ],
        "tokenizer_eos_token": [
            "</s>",
            "2"
        ],
        "tokenizer_bos_token": [
            "<s>",
            "1"
        ],
        "eot_token_id": 2,
        "max_length": 2048
    },
    "bloom-1b1_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.3010527192280794,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=bigscience/bloom-1b1",
            "model_num_parameters": 1065314304,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "eb3dd7399312f5f94fd13f41d2f318117d3eb1e4",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:1",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721454000.547258,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<pad>",
            "3"
        ],
        "tokenizer_eos_token": [
            "</s>",
            "2"
        ],
        "tokenizer_bos_token": [
            "<s>",
            "1"
        ],
        "eot_token_id": 2,
        "max_length": 2048
    },
    "bloom-1b7_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.25345392394245836,
                "acc_stderr,none": 0.0036686079796690346,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.25802337938363445,
                "acc_stderr,none": 0.00637314788725867,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.1746031746031746,
                "acc_stderr,none": 0.033954900208561095
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.2545454545454545,
                "acc_stderr,none": 0.03401506715249039
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.25,
                "acc_stderr,none": 0.03039153369274154
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.2616033755274262,
                "acc_stderr,none": 0.028609516716994934
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.30578512396694213,
                "acc_stderr,none": 0.04205953933884123
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.3333333333333333,
                "acc_stderr,none": 0.04557239513497752
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.3312883435582822,
                "acc_stderr,none": 0.03697983910025588
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.2947976878612717,
                "acc_stderr,none": 0.024547617794803835
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.2435754189944134,
                "acc_stderr,none": 0.01435591196476786
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.2090032154340836,
                "acc_stderr,none": 0.023093140398374224
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.2654320987654321,
                "acc_stderr,none": 0.024569223600460845
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.2561929595827901,
                "acc_stderr,none": 0.011149173153110582
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.26900584795321636,
                "acc_stderr,none": 0.03401052620104089
            },
            "mmlu_other": {
                "acc,none": 0.2520115867396202,
                "acc_stderr,none": 0.007777561246456288,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.25,
                "acc_stderr,none": 0.04351941398892446
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.2490566037735849,
                "acc_stderr,none": 0.026616482980501704
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.23121387283236994,
                "acc_stderr,none": 0.0321473730202947
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.242152466367713,
                "acc_stderr,none": 0.028751392398694755
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.21359223300970873,
                "acc_stderr,none": 0.04058042015646035
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.26495726495726496,
                "acc_stderr,none": 0.02891120880274948
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.31,
                "acc_stderr,none": 0.04648231987117316
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.2886334610472541,
                "acc_stderr,none": 0.016203792703197786
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.25163398692810457,
                "acc_stderr,none": 0.024848018263875192
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.22340425531914893,
                "acc_stderr,none": 0.024847921358063962
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.16544117647058823,
                "acc_stderr,none": 0.02257177102549476
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.25301204819277107,
                "acc_stderr,none": 0.033844291552331346
            },
            "mmlu_social_sciences": {
                "acc,none": 0.24341891452713682,
                "acc_stderr,none": 0.007738718682469128,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.22807017543859648,
                "acc_stderr,none": 0.03947152782669415
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.25757575757575757,
                "acc_stderr,none": 0.031156269519646836
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.19689119170984457,
                "acc_stderr,none": 0.028697873971860695
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.23846153846153847,
                "acc_stderr,none": 0.021606294494647727
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.21428571428571427,
                "acc_stderr,none": 0.026653531596715484
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.23486238532110093,
                "acc_stderr,none": 0.018175110510343578
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.2824427480916031,
                "acc_stderr,none": 0.03948406125768361
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.2647058823529412,
                "acc_stderr,none": 0.017848089574913222
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.3,
                "acc_stderr,none": 0.04389311454644286
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.22040816326530613,
                "acc_stderr,none": 0.026537045312145287
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.26865671641791045,
                "acc_stderr,none": 0.03134328358208954
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.22,
                "acc_stderr,none": 0.04163331998932269
            },
            "mmlu_stem": {
                "acc,none": 0.25784966698382494,
                "acc_stderr,none": 0.00779243787315874,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.24,
                "acc_stderr,none": 0.04292346959909284
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.34814814814814815,
                "acc_stderr,none": 0.041153246103369526
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.24342105263157895,
                "acc_stderr,none": 0.034923496688842384
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2708333333333333,
                "acc_stderr,none": 0.03716177437566016
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.19,
                "acc_stderr,none": 0.03942772444036624
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.27,
                "acc_stderr,none": 0.044619604333847415
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.32,
                "acc_stderr,none": 0.046882617226215034
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.21568627450980393,
                "acc_stderr,none": 0.04092563958237655
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.26,
                "acc_stderr,none": 0.044084400227680794
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.2425531914893617,
                "acc_stderr,none": 0.028020226271200217
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.25517241379310346,
                "acc_stderr,none": 0.03632984052707842
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.2275132275132275,
                "acc_stderr,none": 0.021591269407823774
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.24838709677419354,
                "acc_stderr,none": 0.024580028921480996
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.27586206896551724,
                "acc_stderr,none": 0.03144712581678241
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.33,
                "acc_stderr,none": 0.047258156262526045
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.27037037037037037,
                "acc_stderr,none": 0.02708037281514566
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.25165562913907286,
                "acc_stderr,none": 0.03543304234389985
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.24537037037037038,
                "acc_stderr,none": 0.02934666509437295
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.26785714285714285,
                "acc_stderr,none": 0.04203277291467762
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.25345392394245836,
                "acc_stderr,none": 0.0036686079796690346,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.25802337938363445,
                "acc_stderr,none": 0.00637314788725867,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.2520115867396202,
                "acc_stderr,none": 0.007777561246456288,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.24341891452713682,
                "acc_stderr,none": 0.007738718682469128,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.25784966698382494,
                "acc_stderr,none": 0.00779243787315874,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=bigscience/bloom-1b7",
            "model_num_parameters": 1722408960,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "cc72a88036c2fb937d65efeacc57a0c2ef5d6fe5",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:1",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721454530.6202433,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<pad>",
            "3"
        ],
        "tokenizer_eos_token": [
            "</s>",
            "2"
        ],
        "tokenizer_bos_token": [
            "<s>",
            "1"
        ],
        "eot_token_id": 2,
        "max_length": 2048
    },
    "bloom-1b7_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.32242191113075996,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=bigscience/bloom-1b7",
            "model_num_parameters": 1722408960,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "cc72a88036c2fb937d65efeacc57a0c2ef5d6fe5",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:1",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721454449.7076585,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<pad>",
            "3"
        ],
        "tokenizer_eos_token": [
            "</s>",
            "2"
        ],
        "tokenizer_bos_token": [
            "<s>",
            "1"
        ],
        "eot_token_id": 2,
        "max_length": 2048
    },
    "bloom-3b_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.2593647628542943,
                "acc_stderr,none": 0.003695702716317818,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.27587672688629117,
                "acc_stderr,none": 0.006516704773550319,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.23809523809523808,
                "acc_stderr,none": 0.03809523809523811
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.30303030303030304,
                "acc_stderr,none": 0.03588624800091707
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.2696078431372549,
                "acc_stderr,none": 0.031145570659486782
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.25316455696202533,
                "acc_stderr,none": 0.028304657943035286
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.256198347107438,
                "acc_stderr,none": 0.03984979653302872
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.2962962962962963,
                "acc_stderr,none": 0.04414343666854933
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.27607361963190186,
                "acc_stderr,none": 0.03512385283705051
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.2947976878612717,
                "acc_stderr,none": 0.02454761779480383
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.24804469273743016,
                "acc_stderr,none": 0.01444415780826144
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.24437299035369775,
                "acc_stderr,none": 0.024406162094668907
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.30246913580246915,
                "acc_stderr,none": 0.02555765398186805
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.28878748370273793,
                "acc_stderr,none": 0.011574914757219943
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.3157894736842105,
                "acc_stderr,none": 0.035650796707083106
            },
            "mmlu_other": {
                "acc,none": 0.24943675571290633,
                "acc_stderr,none": 0.007747131609885702,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.22,
                "acc_stderr,none": 0.04163331998932268
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.23018867924528302,
                "acc_stderr,none": 0.02590789712240817
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.24277456647398843,
                "acc_stderr,none": 0.0326926380614177
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.33,
                "acc_stderr,none": 0.04725815626252604
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.23318385650224216,
                "acc_stderr,none": 0.028380391147094716
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.17475728155339806,
                "acc_stderr,none": 0.03760178006026621
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.2606837606837607,
                "acc_stderr,none": 0.028760348956523414
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.31,
                "acc_stderr,none": 0.04648231987117316
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.2707535121328225,
                "acc_stderr,none": 0.01588988836256049
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.27450980392156865,
                "acc_stderr,none": 0.02555316999182652
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.25886524822695034,
                "acc_stderr,none": 0.026129572527180848
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.15808823529411764,
                "acc_stderr,none": 0.02216146260806852
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.25903614457831325,
                "acc_stderr,none": 0.034106466140718564
            },
            "mmlu_social_sciences": {
                "acc,none": 0.23431914202144946,
                "acc_stderr,none": 0.007642636469476552,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.21929824561403508,
                "acc_stderr,none": 0.03892431106518754
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.22727272727272727,
                "acc_stderr,none": 0.02985751567338641
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.22279792746113988,
                "acc_stderr,none": 0.03003114797764154
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.2205128205128205,
                "acc_stderr,none": 0.02102067268082791
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.21428571428571427,
                "acc_stderr,none": 0.026653531596715473
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.22752293577981653,
                "acc_stderr,none": 0.0179744635787765
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.22137404580152673,
                "acc_stderr,none": 0.0364129708131373
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.2630718954248366,
                "acc_stderr,none": 0.017812676542320664
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.23636363636363636,
                "acc_stderr,none": 0.040693063197213754
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.2163265306122449,
                "acc_stderr,none": 0.026358916334904017
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.24378109452736318,
                "acc_stderr,none": 0.03036049015401464
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.29,
                "acc_stderr,none": 0.04560480215720684
            },
            "mmlu_stem": {
                "acc,none": 0.2689502061528703,
                "acc_stderr,none": 0.00790085984711093,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.27,
                "acc_stderr,none": 0.044619604333847394
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.31851851851851853,
                "acc_stderr,none": 0.04024778401977111
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.3157894736842105,
                "acc_stderr,none": 0.03782728980865469
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.24305555555555555,
                "acc_stderr,none": 0.0358687928008034
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.15,
                "acc_stderr,none": 0.035887028128263714
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.31,
                "acc_stderr,none": 0.04648231987117316
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.27,
                "acc_stderr,none": 0.04461960433384741
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.2549019607843137,
                "acc_stderr,none": 0.043364327079931785
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.25,
                "acc_stderr,none": 0.04351941398892446
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.2723404255319149,
                "acc_stderr,none": 0.029101290698386687
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2689655172413793,
                "acc_stderr,none": 0.036951833116502325
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.2724867724867725,
                "acc_stderr,none": 0.02293097307163334
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.24838709677419354,
                "acc_stderr,none": 0.024580028921480992
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.2857142857142857,
                "acc_stderr,none": 0.03178529710642749
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.33,
                "acc_stderr,none": 0.04725815626252604
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.2777777777777778,
                "acc_stderr,none": 0.027309140588230182
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.25165562913907286,
                "acc_stderr,none": 0.035433042343899844
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.25462962962962965,
                "acc_stderr,none": 0.029711275860005344
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.25892857142857145,
                "acc_stderr,none": 0.04157751539865629
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.2593647628542943,
                "acc_stderr,none": 0.003695702716317818,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.27587672688629117,
                "acc_stderr,none": 0.006516704773550319,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.24943675571290633,
                "acc_stderr,none": 0.007747131609885702,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.23431914202144946,
                "acc_stderr,none": 0.007642636469476552,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.2689502061528703,
                "acc_stderr,none": 0.00790085984711093,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=bigscience/bloom-3b",
            "model_num_parameters": 3002557440,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "52bc5b43010b4844513826b8be3f78c7344c37d7",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:1",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721455033.752783,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<pad>",
            "3"
        ],
        "tokenizer_eos_token": [
            "</s>",
            "2"
        ],
        "tokenizer_bos_token": [
            "<s>",
            "1"
        ],
        "eot_token_id": 2,
        "max_length": 2048
    },
    "bloom-3b_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.3046361815020503,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=bigscience/bloom-3b",
            "model_num_parameters": 3002557440,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "52bc5b43010b4844513826b8be3f78c7344c37d7",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:1",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721454947.300144,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<pad>",
            "3"
        ],
        "tokenizer_eos_token": [
            "</s>",
            "2"
        ],
        "tokenizer_bos_token": [
            "<s>",
            "1"
        ],
        "eot_token_id": 2,
        "max_length": 2048
    },
    "bloom-560m_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.2299529981484119,
                "acc_stderr,none": 0.003545482078953296,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.24229543039319873,
                "acc_stderr,none": 0.006244717285024366,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2857142857142857,
                "acc_stderr,none": 0.04040610178208841
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.21818181818181817,
                "acc_stderr,none": 0.03225078108306289
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.25,
                "acc_stderr,none": 0.03039153369274154
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.270042194092827,
                "acc_stderr,none": 0.028900721906293426
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.2396694214876033,
                "acc_stderr,none": 0.03896878985070417
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.25925925925925924,
                "acc_stderr,none": 0.04236511258094634
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.22085889570552147,
                "acc_stderr,none": 0.032591773927421776
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.24855491329479767,
                "acc_stderr,none": 0.023267528432100174
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.23798882681564246,
                "acc_stderr,none": 0.014242630070574885
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.1864951768488746,
                "acc_stderr,none": 0.02212243977248077
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.2191358024691358,
                "acc_stderr,none": 0.023016705640262203
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.2457627118644068,
                "acc_stderr,none": 0.01099615663514269
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.3216374269005848,
                "acc_stderr,none": 0.03582529442573122
            },
            "mmlu_other": {
                "acc,none": 0.2404248471194078,
                "acc_stderr,none": 0.007650080721986917,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.2188679245283019,
                "acc_stderr,none": 0.025447863825108604
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.20809248554913296,
                "acc_stderr,none": 0.030952890217749884
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.18,
                "acc_stderr,none": 0.038612291966536955
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.31390134529147984,
                "acc_stderr,none": 0.03114679648297246
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.17475728155339806,
                "acc_stderr,none": 0.03760178006026621
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.2905982905982906,
                "acc_stderr,none": 0.029745048572674057
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.31,
                "acc_stderr,none": 0.04648231987117316
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.23499361430395913,
                "acc_stderr,none": 0.01516202415227843
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.22549019607843138,
                "acc_stderr,none": 0.023929155517351284
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.23404255319148937,
                "acc_stderr,none": 0.025257861359432407
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.19117647058823528,
                "acc_stderr,none": 0.02388688192244036
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.28313253012048195,
                "acc_stderr,none": 0.03507295431370518
            },
            "mmlu_social_sciences": {
                "acc,none": 0.2170945726356841,
                "acc_stderr,none": 0.007428786285788534,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.23684210526315788,
                "acc_stderr,none": 0.039994238792813386
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.17676767676767677,
                "acc_stderr,none": 0.027178752639044915
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.19689119170984457,
                "acc_stderr,none": 0.02869787397186069
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.20256410256410257,
                "acc_stderr,none": 0.020377660970371397
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.21008403361344538,
                "acc_stderr,none": 0.026461398717471874
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.1926605504587156,
                "acc_stderr,none": 0.016909276884936073
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.2595419847328244,
                "acc_stderr,none": 0.03844876139785271
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.25,
                "acc_stderr,none": 0.01751781884501444
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.21818181818181817,
                "acc_stderr,none": 0.03955932861795833
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.18775510204081633,
                "acc_stderr,none": 0.02500025603954622
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.24378109452736318,
                "acc_stderr,none": 0.030360490154014652
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.28,
                "acc_stderr,none": 0.045126085985421276
            },
            "mmlu_stem": {
                "acc,none": 0.21376466856961623,
                "acc_stderr,none": 0.007286294199311507,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.22,
                "acc_stderr,none": 0.04163331998932269
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.1925925925925926,
                "acc_stderr,none": 0.03406542058502653
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.17763157894736842,
                "acc_stderr,none": 0.031103182383123398
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2569444444444444,
                "acc_stderr,none": 0.03653946969442099
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.25,
                "acc_stderr,none": 0.04351941398892446
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.21568627450980393,
                "acc_stderr,none": 0.040925639582376556
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.29,
                "acc_stderr,none": 0.045604802157206845
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.26382978723404255,
                "acc_stderr,none": 0.02880998985410298
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2413793103448276,
                "acc_stderr,none": 0.03565998174135302
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.20899470899470898,
                "acc_stderr,none": 0.020940481565334835
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.17096774193548386,
                "acc_stderr,none": 0.02141724293632157
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.15763546798029557,
                "acc_stderr,none": 0.0256390141311724
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.25,
                "acc_stderr,none": 0.04351941398892446
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.21481481481481482,
                "acc_stderr,none": 0.025040443877000683
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.1986754966887417,
                "acc_stderr,none": 0.032578473844367746
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.1574074074074074,
                "acc_stderr,none": 0.024837173518242384
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.32142857142857145,
                "acc_stderr,none": 0.04432804055291519
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.2299529981484119,
                "acc_stderr,none": 0.003545482078953296,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.24229543039319873,
                "acc_stderr,none": 0.006244717285024366,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.2404248471194078,
                "acc_stderr,none": 0.007650080721986917,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.2170945726356841,
                "acc_stderr,none": 0.007428786285788534,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.21376466856961623,
                "acc_stderr,none": 0.007286294199311507,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=bigscience/bloom-560m",
            "model_num_parameters": 559214592,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:1",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721453611.9698822,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<pad>",
            "3"
        ],
        "tokenizer_eos_token": [
            "</s>",
            "2"
        ],
        "tokenizer_bos_token": [
            "<s>",
            "1"
        ],
        "eot_token_id": 2,
        "max_length": 2048
    },
    "bloom-560m_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.29252328716626874,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=bigscience/bloom-560m",
            "model_num_parameters": 559214592,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:1",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721453576.4417913,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<pad>",
            "3"
        ],
        "tokenizer_eos_token": [
            "</s>",
            "2"
        ],
        "tokenizer_bos_token": [
            "<s>",
            "1"
        ],
        "eot_token_id": 2,
        "max_length": 2048
    },
    "bloom-7b1_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.2639225181598063,
                "acc_stderr,none": 0.0037146568078992616,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.25866099893730077,
                "acc_stderr,none": 0.006381154058764947,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.29365079365079366,
                "acc_stderr,none": 0.04073524322147127
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.24242424242424243,
                "acc_stderr,none": 0.03346409881055953
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.22058823529411764,
                "acc_stderr,none": 0.029102254389674096
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.23628691983122363,
                "acc_stderr,none": 0.02765215314415926
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.2892561983471074,
                "acc_stderr,none": 0.04139112727635464
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.21296296296296297,
                "acc_stderr,none": 0.039578354719809784
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.2085889570552147,
                "acc_stderr,none": 0.03192193448934723
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.2514450867052023,
                "acc_stderr,none": 0.023357365785874037
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.27262569832402234,
                "acc_stderr,none": 0.014893391735249617
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.22508038585209003,
                "acc_stderr,none": 0.023720088516179034
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.21604938271604937,
                "acc_stderr,none": 0.02289916291844577
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.2816166883963494,
                "acc_stderr,none": 0.011487783272786696
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.2573099415204678,
                "acc_stderr,none": 0.03352799844161865
            },
            "mmlu_other": {
                "acc,none": 0.2417122626327647,
                "acc_stderr,none": 0.007668028832844002,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.22,
                "acc_stderr,none": 0.04163331998932269
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.2792452830188679,
                "acc_stderr,none": 0.027611163402399715
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.2774566473988439,
                "acc_stderr,none": 0.03414014007044036
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.19,
                "acc_stderr,none": 0.03942772444036625
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.17937219730941703,
                "acc_stderr,none": 0.025749819569192787
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.30097087378640774,
                "acc_stderr,none": 0.04541609446503948
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.26495726495726496,
                "acc_stderr,none": 0.02891120880274948
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.29,
                "acc_stderr,none": 0.04560480215720684
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.2222222222222222,
                "acc_stderr,none": 0.0148668216647096
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.2875816993464052,
                "acc_stderr,none": 0.02591780611714716
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.23404255319148937,
                "acc_stderr,none": 0.025257861359432407
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.2536764705882353,
                "acc_stderr,none": 0.02643132987078954
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.1746987951807229,
                "acc_stderr,none": 0.029560326211256854
            },
            "mmlu_social_sciences": {
                "acc,none": 0.2853428664283393,
                "acc_stderr,none": 0.008137766534113513,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.2719298245614035,
                "acc_stderr,none": 0.041857744240220575
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.29797979797979796,
                "acc_stderr,none": 0.03258630383836556
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.3160621761658031,
                "acc_stderr,none": 0.03355397369686173
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.28205128205128205,
                "acc_stderr,none": 0.022815813098896603
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.28991596638655465,
                "acc_stderr,none": 0.029472485833136094
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.30275229357798167,
                "acc_stderr,none": 0.019698711434756353
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.25190839694656486,
                "acc_stderr,none": 0.03807387116306086
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.2630718954248366,
                "acc_stderr,none": 0.017812676542320657
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.23636363636363636,
                "acc_stderr,none": 0.040693063197213754
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.35918367346938773,
                "acc_stderr,none": 0.030713560455108493
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.23880597014925373,
                "acc_stderr,none": 0.03014777593540922
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.27,
                "acc_stderr,none": 0.0446196043338474
            },
            "mmlu_stem": {
                "acc,none": 0.272756105296543,
                "acc_stderr,none": 0.007927790636571527,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.21,
                "acc_stderr,none": 0.04093601807403326
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.24444444444444444,
                "acc_stderr,none": 0.037125378336148665
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.27631578947368424,
                "acc_stderr,none": 0.03639057569952924
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.25,
                "acc_stderr,none": 0.03621034121889507
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.38,
                "acc_stderr,none": 0.048783173121456316
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.28,
                "acc_stderr,none": 0.04512608598542127
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.31,
                "acc_stderr,none": 0.04648231987117316
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.22549019607843138,
                "acc_stderr,none": 0.041583075330832865
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.23,
                "acc_stderr,none": 0.042295258468165044
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.3276595744680851,
                "acc_stderr,none": 0.030683020843231004
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.22758620689655173,
                "acc_stderr,none": 0.03493950380131184
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.2566137566137566,
                "acc_stderr,none": 0.022494510767503154
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.27741935483870966,
                "acc_stderr,none": 0.025470196835900055
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.2857142857142857,
                "acc_stderr,none": 0.0317852971064275
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.27,
                "acc_stderr,none": 0.0446196043338474
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.22962962962962963,
                "acc_stderr,none": 0.025644108639267624
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.31125827814569534,
                "acc_stderr,none": 0.037804458505267334
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.30092592592592593,
                "acc_stderr,none": 0.031280390843298804
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.29464285714285715,
                "acc_stderr,none": 0.043270409325787296
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.2639225181598063,
                "acc_stderr,none": 0.0037146568078992616,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.25866099893730077,
                "acc_stderr,none": 0.006381154058764947,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.2417122626327647,
                "acc_stderr,none": 0.007668028832844002,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.2853428664283393,
                "acc_stderr,none": 0.008137766534113513,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.272756105296543,
                "acc_stderr,none": 0.007927790636571527,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=bigscience/bloom-7b1",
            "model_num_parameters": 7069016064,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "6232703e399354503377bf59dfbb8397fd569e4a",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:1",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721456960.3296013,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<pad>",
            "3"
        ],
        "tokenizer_eos_token": [
            "</s>",
            "2"
        ],
        "tokenizer_bos_token": [
            "<s>",
            "1"
        ],
        "eot_token_id": 2,
        "max_length": 2048
    },
    "bloom-7b1_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.30572584220409466,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=bigscience/bloom-7b1",
            "model_num_parameters": 7069016064,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "6232703e399354503377bf59dfbb8397fd569e4a",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:1",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721455527.3346217,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<pad>",
            "3"
        ],
        "tokenizer_eos_token": [
            "</s>",
            "2"
        ],
        "tokenizer_bos_token": [
            "<s>",
            "1"
        ],
        "eot_token_id": 2,
        "max_length": 2048
    },
    "pythia-1.4b_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.24398233869819114,
                "acc_stderr,none": 0.0036172771960345093,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.24463336875664188,
                "acc_stderr,none": 0.006263932424328401,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2777777777777778,
                "acc_stderr,none": 0.040061680838488774
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.2,
                "acc_stderr,none": 0.03123475237772117
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.25980392156862747,
                "acc_stderr,none": 0.030778554678693247
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.28270042194092826,
                "acc_stderr,none": 0.029312814153955914
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.2809917355371901,
                "acc_stderr,none": 0.041032038305145124
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.28703703703703703,
                "acc_stderr,none": 0.04373313040914761
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.24539877300613497,
                "acc_stderr,none": 0.03380939813943354
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.23410404624277456,
                "acc_stderr,none": 0.022797110278071138
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.23798882681564246,
                "acc_stderr,none": 0.014242630070574885
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.1864951768488746,
                "acc_stderr,none": 0.02212243977248077
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.2345679012345679,
                "acc_stderr,none": 0.023576881744005716
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.24511082138200782,
                "acc_stderr,none": 0.010986307870045514
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.3157894736842105,
                "acc_stderr,none": 0.035650796707083134
            },
            "mmlu_other": {
                "acc,none": 0.2597360798197618,
                "acc_stderr,none": 0.00782523251328105,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.32,
                "acc_stderr,none": 0.04688261722621505
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.28679245283018867,
                "acc_stderr,none": 0.027834912527544074
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.19653179190751446,
                "acc_stderr,none": 0.030299574664788147
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.34,
                "acc_stderr,none": 0.047609522856952344
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.3542600896860987,
                "acc_stderr,none": 0.032100621541349864
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.1262135922330097,
                "acc_stderr,none": 0.032881802788086285
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.2905982905982906,
                "acc_stderr,none": 0.029745048572674043
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.32,
                "acc_stderr,none": 0.046882617226215034
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.24521072796934865,
                "acc_stderr,none": 0.015384352284543932
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.2549019607843137,
                "acc_stderr,none": 0.02495418432487991
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.25886524822695034,
                "acc_stderr,none": 0.026129572527180848
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.17279411764705882,
                "acc_stderr,none": 0.022966067585581788
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.29518072289156627,
                "acc_stderr,none": 0.03550920185689631
            },
            "mmlu_social_sciences": {
                "acc,none": 0.2378940526486838,
                "acc_stderr,none": 0.007669368476217683,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.20175438596491227,
                "acc_stderr,none": 0.03775205013583638
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.1919191919191919,
                "acc_stderr,none": 0.028057791672989017
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.20207253886010362,
                "acc_stderr,none": 0.02897908979429673
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.21794871794871795,
                "acc_stderr,none": 0.020932445774463175
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.23109243697478993,
                "acc_stderr,none": 0.02738140692786896
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.24587155963302754,
                "acc_stderr,none": 0.018461940968708457
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.26717557251908397,
                "acc_stderr,none": 0.03880848301082396
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.2581699346405229,
                "acc_stderr,none": 0.01770453165325007
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.32727272727272727,
                "acc_stderr,none": 0.04494290866252088
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.2,
                "acc_stderr,none": 0.025607375986579153
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.24378109452736318,
                "acc_stderr,none": 0.030360490154014652
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.31,
                "acc_stderr,none": 0.04648231987117316
            },
            "mmlu_stem": {
                "acc,none": 0.23342848081192516,
                "acc_stderr,none": 0.0075279017029330954,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.24,
                "acc_stderr,none": 0.04292346959909284
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.2074074074074074,
                "acc_stderr,none": 0.03502553170678318
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.18421052631578946,
                "acc_stderr,none": 0.0315469804508223
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2708333333333333,
                "acc_stderr,none": 0.037161774375660164
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.18,
                "acc_stderr,none": 0.03861229196653695
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.25,
                "acc_stderr,none": 0.04351941398892446
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.22549019607843138,
                "acc_stderr,none": 0.041583075330832865
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.27,
                "acc_stderr,none": 0.044619604333847394
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.2723404255319149,
                "acc_stderr,none": 0.029101290698386694
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2827586206896552,
                "acc_stderr,none": 0.037528339580033376
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.24603174603174602,
                "acc_stderr,none": 0.022182037202948368
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.20967741935483872,
                "acc_stderr,none": 0.023157879349083532
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.18226600985221675,
                "acc_stderr,none": 0.02716334085964515
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.23,
                "acc_stderr,none": 0.04229525846816506
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.23333333333333334,
                "acc_stderr,none": 0.02578787422095932
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.2119205298013245,
                "acc_stderr,none": 0.03336767086567977
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.18981481481481483,
                "acc_stderr,none": 0.026744714834691912
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.3125,
                "acc_stderr,none": 0.043994650575715215
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.24398233869819114,
                "acc_stderr,none": 0.0036172771960345093,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.24463336875664188,
                "acc_stderr,none": 0.006263932424328401,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.2597360798197618,
                "acc_stderr,none": 0.00782523251328105,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.2378940526486838,
                "acc_stderr,none": 0.007669368476217683,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.23342848081192516,
                "acc_stderr,none": 0.0075279017029330954,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=EleutherAI/pythia-1.4b",
            "model_num_parameters": 1414647808,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "fedc38a16eea3bd36a96b906d78d11d2ce18ed79",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721402885.9771802,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "0"
        ],
        "eot_token_id": 0,
        "max_length": 2048
    },
    "pythia-1.4b_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.32741005467065865,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=EleutherAI/pythia-1.4b",
            "model_num_parameters": 1414647808,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "fedc38a16eea3bd36a96b906d78d11d2ce18ed79",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721390530.1260178,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "0"
        ],
        "eot_token_id": 0,
        "max_length": 2048
    },
    "pythia-12b_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.2388548639794901,
                "acc_stderr,none": 0.003593649076616158,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.2516471838469713,
                "acc_stderr,none": 0.0063266777648673085,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2857142857142857,
                "acc_stderr,none": 0.0404061017820884
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.21818181818181817,
                "acc_stderr,none": 0.03225078108306289
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.23529411764705882,
                "acc_stderr,none": 0.029771775228145617
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.27848101265822783,
                "acc_stderr,none": 0.029178682304842562
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.24793388429752067,
                "acc_stderr,none": 0.03941897526516303
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.2962962962962963,
                "acc_stderr,none": 0.04414343666854933
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.2883435582822086,
                "acc_stderr,none": 0.035590395316173425
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.2630057803468208,
                "acc_stderr,none": 0.023703099525258172
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.24916201117318434,
                "acc_stderr,none": 0.014465893829859924
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.22508038585209003,
                "acc_stderr,none": 0.02372008851617903
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.2222222222222222,
                "acc_stderr,none": 0.023132376234543343
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.24641460234680573,
                "acc_stderr,none": 0.011005971399927242
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.3216374269005848,
                "acc_stderr,none": 0.035825294425731215
            },
            "mmlu_other": {
                "acc,none": 0.23591889282265852,
                "acc_stderr,none": 0.007582564304229218,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.29,
                "acc_stderr,none": 0.045604802157206845
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.2188679245283019,
                "acc_stderr,none": 0.02544786382510861
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.1676300578034682,
                "acc_stderr,none": 0.028481963032143377
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.19,
                "acc_stderr,none": 0.03942772444036623
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.3452914798206278,
                "acc_stderr,none": 0.03191100192835795
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.1650485436893204,
                "acc_stderr,none": 0.036756688322331886
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.2692307692307692,
                "acc_stderr,none": 0.02905858830374884
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.26,
                "acc_stderr,none": 0.04408440022768078
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.2503192848020434,
                "acc_stderr,none": 0.015491088951494586
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.20261437908496732,
                "acc_stderr,none": 0.023015446877985662
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.23049645390070922,
                "acc_stderr,none": 0.025123739226872402
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.16176470588235295,
                "acc_stderr,none": 0.02236867256288675
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.2891566265060241,
                "acc_stderr,none": 0.03529486801511115
            },
            "mmlu_social_sciences": {
                "acc,none": 0.22586935326616833,
                "acc_stderr,none": 0.007535862515162296,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.19298245614035087,
                "acc_stderr,none": 0.03712454853721368
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.20707070707070707,
                "acc_stderr,none": 0.028869778460267042
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.22797927461139897,
                "acc_stderr,none": 0.030276909945178277
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.2358974358974359,
                "acc_stderr,none": 0.021525965407408726
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.23529411764705882,
                "acc_stderr,none": 0.02755361446786381
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.2,
                "acc_stderr,none": 0.017149858514250958
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.20610687022900764,
                "acc_stderr,none": 0.035477710041594626
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.26143790849673204,
                "acc_stderr,none": 0.01777694715752803
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.24545454545454545,
                "acc_stderr,none": 0.041220665028782834
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.17142857142857143,
                "acc_stderr,none": 0.024127463462650146
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.23880597014925373,
                "acc_stderr,none": 0.030147775935409224
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.27,
                "acc_stderr,none": 0.04461960433384741
            },
            "mmlu_stem": {
                "acc,none": 0.2353314303837615,
                "acc_stderr,none": 0.007556006157022096,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.2,
                "acc_stderr,none": 0.04020151261036843
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.24444444444444444,
                "acc_stderr,none": 0.03712537833614866
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.20394736842105263,
                "acc_stderr,none": 0.032790004063100515
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.25,
                "acc_stderr,none": 0.03621034121889507
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.21,
                "acc_stderr,none": 0.04093601807403325
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.22,
                "acc_stderr,none": 0.04163331998932269
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.24,
                "acc_stderr,none": 0.04292346959909282
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.21568627450980393,
                "acc_stderr,none": 0.04092563958237654
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.24,
                "acc_stderr,none": 0.04292346959909281
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.2936170212765957,
                "acc_stderr,none": 0.029771642712491223
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.1793103448275862,
                "acc_stderr,none": 0.03196766433373187
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.2566137566137566,
                "acc_stderr,none": 0.022494510767503157
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.22903225806451613,
                "acc_stderr,none": 0.023904914311782648
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.20689655172413793,
                "acc_stderr,none": 0.02850137816789395
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.28,
                "acc_stderr,none": 0.045126085985421276
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.24074074074074073,
                "acc_stderr,none": 0.026067159222275805
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.2052980132450331,
                "acc_stderr,none": 0.03297986648473836
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.20833333333333334,
                "acc_stderr,none": 0.027696910713093936
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.3125,
                "acc_stderr,none": 0.043994650575715215
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.2388548639794901,
                "acc_stderr,none": 0.003593649076616158,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.2516471838469713,
                "acc_stderr,none": 0.0063266777648673085,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.23591889282265852,
                "acc_stderr,none": 0.007582564304229218,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.22586935326616833,
                "acc_stderr,none": 0.007535862515162296,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.2353314303837615,
                "acc_stderr,none": 0.007556006157022096,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=EleutherAI/pythia-12b",
            "model_num_parameters": 11846072320,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "bb1e3e710cdf6b524461d543cfb5ba773f0a81b6",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721409253.5720065,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "0"
        ],
        "eot_token_id": 0,
        "max_length": 2048
    },
    "pythia-12b_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.30031691749868283,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=EleutherAI/pythia-12b",
            "model_num_parameters": 11846072320,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "bb1e3e710cdf6b524461d543cfb5ba773f0a81b6",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721390845.8667135,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "0"
        ],
        "eot_token_id": 0,
        "max_length": 2048
    },
    "pythia-160m_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.23009542800170915,
                "acc_stderr,none": 0.0035464532260202344,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.24165781083953242,
                "acc_stderr,none": 0.0062390731532646826,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2698412698412698,
                "acc_stderr,none": 0.03970158273235172
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.21818181818181817,
                "acc_stderr,none": 0.03225078108306289
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.25,
                "acc_stderr,none": 0.03039153369274154
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.270042194092827,
                "acc_stderr,none": 0.028900721906293426
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.2396694214876033,
                "acc_stderr,none": 0.03896878985070417
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.26851851851851855,
                "acc_stderr,none": 0.04284467968052192
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.22085889570552147,
                "acc_stderr,none": 0.032591773927421776
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.24277456647398843,
                "acc_stderr,none": 0.023083658586984204
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.23798882681564246,
                "acc_stderr,none": 0.014242630070574885
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.1864951768488746,
                "acc_stderr,none": 0.02212243977248077
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.21296296296296297,
                "acc_stderr,none": 0.022779719088733396
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.2470664928292047,
                "acc_stderr,none": 0.011015752255279329
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.3216374269005848,
                "acc_stderr,none": 0.03582529442573122
            },
            "mmlu_other": {
                "acc,none": 0.23913743160605086,
                "acc_stderr,none": 0.007634770066825422,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.21132075471698114,
                "acc_stderr,none": 0.025125766484827856
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.20809248554913296,
                "acc_stderr,none": 0.030952890217749888
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.18,
                "acc_stderr,none": 0.038612291966536955
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.3183856502242152,
                "acc_stderr,none": 0.03126580522513713
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.17475728155339806,
                "acc_stderr,none": 0.03760178006026621
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.2905982905982906,
                "acc_stderr,none": 0.029745048572674057
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.23499361430395913,
                "acc_stderr,none": 0.01516202415227843
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.2222222222222222,
                "acc_stderr,none": 0.023805186524888142
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.23049645390070922,
                "acc_stderr,none": 0.025123739226872405
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.19117647058823528,
                "acc_stderr,none": 0.02388688192244036
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.28313253012048195,
                "acc_stderr,none": 0.03507295431370518
            },
            "mmlu_social_sciences": {
                "acc,none": 0.2183945401364966,
                "acc_stderr,none": 0.007443888916164822,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.23684210526315788,
                "acc_stderr,none": 0.039994238792813386
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.18181818181818182,
                "acc_stderr,none": 0.027479603010538808
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.19689119170984457,
                "acc_stderr,none": 0.02869787397186069
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.20512820512820512,
                "acc_stderr,none": 0.020473233173551965
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.21008403361344538,
                "acc_stderr,none": 0.026461398717471874
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.1889908256880734,
                "acc_stderr,none": 0.016785481159203624
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.2595419847328244,
                "acc_stderr,none": 0.03844876139785271
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.25326797385620914,
                "acc_stderr,none": 0.017593486895366835
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.21818181818181817,
                "acc_stderr,none": 0.03955932861795833
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.19183673469387755,
                "acc_stderr,none": 0.025206963154225395
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.24875621890547264,
                "acc_stderr,none": 0.03056767593891672
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.28,
                "acc_stderr,none": 0.045126085985421276
            },
            "mmlu_stem": {
                "acc,none": 0.21535045987947987,
                "acc_stderr,none": 0.007307369963880755,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.23,
                "acc_stderr,none": 0.042295258468165065
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.1925925925925926,
                "acc_stderr,none": 0.03406542058502652
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.17763157894736842,
                "acc_stderr,none": 0.031103182383123398
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2638888888888889,
                "acc_stderr,none": 0.03685651095897532
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.2,
                "acc_stderr,none": 0.04020151261036846
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.26,
                "acc_stderr,none": 0.044084400227680794
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.21568627450980393,
                "acc_stderr,none": 0.040925639582376556
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.28,
                "acc_stderr,none": 0.045126085985421276
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.2553191489361702,
                "acc_stderr,none": 0.028504856470514206
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2413793103448276,
                "acc_stderr,none": 0.03565998174135302
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.20899470899470898,
                "acc_stderr,none": 0.020940481565334835
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.1870967741935484,
                "acc_stderr,none": 0.022185710092252262
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.15763546798029557,
                "acc_stderr,none": 0.0256390141311724
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.27,
                "acc_stderr,none": 0.044619604333847394
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.2111111111111111,
                "acc_stderr,none": 0.02488211685765508
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.2052980132450331,
                "acc_stderr,none": 0.032979866484738336
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.1527777777777778,
                "acc_stderr,none": 0.02453632602613422
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.32142857142857145,
                "acc_stderr,none": 0.04432804055291519
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.23009542800170915,
                "acc_stderr,none": 0.0035464532260202344,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.24165781083953242,
                "acc_stderr,none": 0.0062390731532646826,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.23913743160605086,
                "acc_stderr,none": 0.007634770066825422,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.2183945401364966,
                "acc_stderr,none": 0.007443888916164822,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.21535045987947987,
                "acc_stderr,none": 0.007307369963880755,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=EleutherAI/pythia-160m",
            "model_num_parameters": 162322944,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "50f5173d932e8e61f858120bcb800b97af589f46",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721401032.365751,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "0"
        ],
        "eot_token_id": 0,
        "max_length": 2048
    },
    "pythia-160m_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.2728508747777333,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=EleutherAI/pythia-160m",
            "model_num_parameters": 162322944,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "50f5173d932e8e61f858120bcb800b97af589f46",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721389916.5578587,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "0"
        ],
        "eot_token_id": 0,
        "max_length": 2048
    },
    "pythia-1b_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.23308645492095142,
                "acc_stderr,none": 0.0035651566866391647,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.23931987247608927,
                "acc_stderr,none": 0.006219111231247359,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.25396825396825395,
                "acc_stderr,none": 0.03893259610604673
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.21818181818181817,
                "acc_stderr,none": 0.03225078108306289
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.24509803921568626,
                "acc_stderr,none": 0.030190282453501954
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.2489451476793249,
                "acc_stderr,none": 0.028146970599422644
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.2396694214876033,
                "acc_stderr,none": 0.03896878985070417
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.2962962962962963,
                "acc_stderr,none": 0.04414343666854933
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.2392638036809816,
                "acc_stderr,none": 0.03351953879521271
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.24277456647398843,
                "acc_stderr,none": 0.023083658586984204
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.23798882681564246,
                "acc_stderr,none": 0.014242630070574885
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.1864951768488746,
                "acc_stderr,none": 0.02212243977248077
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.21604938271604937,
                "acc_stderr,none": 0.022899162918445813
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.2405475880052151,
                "acc_stderr,none": 0.010916406735478947
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.3216374269005848,
                "acc_stderr,none": 0.03582529442573122
            },
            "mmlu_other": {
                "acc,none": 0.24975860959124557,
                "acc_stderr,none": 0.00776183109034571,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.29,
                "acc_stderr,none": 0.045604802157206845
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.2188679245283019,
                "acc_stderr,none": 0.025447863825108604
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.19653179190751446,
                "acc_stderr,none": 0.030299574664788147
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.22,
                "acc_stderr,none": 0.041633319989322695
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.3094170403587444,
                "acc_stderr,none": 0.03102441174057221
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.1941747572815534,
                "acc_stderr,none": 0.03916667762822584
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.2905982905982906,
                "acc_stderr,none": 0.029745048572674054
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.2413793103448276,
                "acc_stderr,none": 0.015302380123542094
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.22875816993464052,
                "acc_stderr,none": 0.024051029739912255
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.25177304964539005,
                "acc_stderr,none": 0.025892151156709405
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.2647058823529412,
                "acc_stderr,none": 0.026799562024887667
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.26506024096385544,
                "acc_stderr,none": 0.03436024037944967
            },
            "mmlu_social_sciences": {
                "acc,none": 0.21936951576210595,
                "acc_stderr,none": 0.007457733899605765,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.2543859649122807,
                "acc_stderr,none": 0.040969851398436716
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.17676767676767677,
                "acc_stderr,none": 0.027178752639044915
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.19689119170984457,
                "acc_stderr,none": 0.02869787397186069
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.19743589743589743,
                "acc_stderr,none": 0.020182646968674823
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.22268907563025211,
                "acc_stderr,none": 0.027025433498882392
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.20917431192660552,
                "acc_stderr,none": 0.017437937173343236
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.25190839694656486,
                "acc_stderr,none": 0.038073871163060866
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.238562091503268,
                "acc_stderr,none": 0.017242385828779575
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.21818181818181817,
                "acc_stderr,none": 0.03955932861795833
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.18775510204081633,
                "acc_stderr,none": 0.02500025603954622
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.24378109452736318,
                "acc_stderr,none": 0.030360490154014652
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.31,
                "acc_stderr,none": 0.04648231987117316
            },
            "mmlu_stem": {
                "acc,none": 0.22074215033301617,
                "acc_stderr,none": 0.00738217757687677,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.23,
                "acc_stderr,none": 0.04229525846816506
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.1925925925925926,
                "acc_stderr,none": 0.03406542058502652
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.19736842105263158,
                "acc_stderr,none": 0.03238981601699397
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2708333333333333,
                "acc_stderr,none": 0.03716177437566016
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.23,
                "acc_stderr,none": 0.04229525846816506
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.26,
                "acc_stderr,none": 0.044084400227680794
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.21568627450980393,
                "acc_stderr,none": 0.04092563958237656
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.27,
                "acc_stderr,none": 0.044619604333847394
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.26382978723404255,
                "acc_stderr,none": 0.02880998985410298
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.23448275862068965,
                "acc_stderr,none": 0.035306258743465914
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.21693121693121692,
                "acc_stderr,none": 0.02122708244944505
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.18387096774193548,
                "acc_stderr,none": 0.022037217340267846
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.15270935960591134,
                "acc_stderr,none": 0.025308904539380613
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.26,
                "acc_stderr,none": 0.0440844002276808
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.2074074074074074,
                "acc_stderr,none": 0.024720713193952165
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.1986754966887417,
                "acc_stderr,none": 0.03257847384436775
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.2175925925925926,
                "acc_stderr,none": 0.028139689444859648
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.30357142857142855,
                "acc_stderr,none": 0.04364226155841044
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.23308645492095142,
                "acc_stderr,none": 0.0035651566866391647,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.23931987247608927,
                "acc_stderr,none": 0.006219111231247359,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.24975860959124557,
                "acc_stderr,none": 0.00776183109034571,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.21936951576210595,
                "acc_stderr,none": 0.007457733899605765,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.22074215033301617,
                "acc_stderr,none": 0.00738217757687677,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=EleutherAI/pythia-1b",
            "model_num_parameters": 1011781632,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "f73d7dcc545c8bd326d8559c8ef84ffe92fea6b2",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721402247.6865313,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "0"
        ],
        "eot_token_id": 0,
        "max_length": 2048
    },
    "pythia-1b_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.26923896265860153,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=EleutherAI/pythia-1b",
            "model_num_parameters": 1011781632,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "f73d7dcc545c8bd326d8559c8ef84ffe92fea6b2",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721389984.580442,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "0"
        ],
        "eot_token_id": 0,
        "max_length": 2048
    },
    "pythia-2.8b_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.2477567298105683,
                "acc_stderr,none": 0.0036381371880230386,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.23230605738575982,
                "acc_stderr,none": 0.006157628254739183,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.29365079365079366,
                "acc_stderr,none": 0.04073524322147125
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.23030303030303031,
                "acc_stderr,none": 0.03287666758603489
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.21568627450980393,
                "acc_stderr,none": 0.0288674314498493
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.2320675105485232,
                "acc_stderr,none": 0.02747974455080851
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.18181818181818182,
                "acc_stderr,none": 0.03520893951097652
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.23148148148148148,
                "acc_stderr,none": 0.04077494709252628
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.1901840490797546,
                "acc_stderr,none": 0.030833491146281224
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.23121387283236994,
                "acc_stderr,none": 0.022698657167855713
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.2424581005586592,
                "acc_stderr,none": 0.014333522059217887
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.1864951768488746,
                "acc_stderr,none": 0.02212243977248079
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.25308641975308643,
                "acc_stderr,none": 0.024191808600713
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.23728813559322035,
                "acc_stderr,none": 0.01086543669078027
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.23391812865497075,
                "acc_stderr,none": 0.03246721765117827
            },
            "mmlu_other": {
                "acc,none": 0.27421950434502734,
                "acc_stderr,none": 0.007995123598653522,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.19,
                "acc_stderr,none": 0.03942772444036623
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.27169811320754716,
                "acc_stderr,none": 0.027377706624670713
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.2543352601156069,
                "acc_stderr,none": 0.0332055644308557
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.27,
                "acc_stderr,none": 0.0446196043338474
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.2914798206278027,
                "acc_stderr,none": 0.030500283176545906
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.27184466019417475,
                "acc_stderr,none": 0.044052680241409216
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.28205128205128205,
                "acc_stderr,none": 0.02948036054954119
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.28,
                "acc_stderr,none": 0.04512608598542127
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.26309067688378035,
                "acc_stderr,none": 0.015745497169049046
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.23202614379084968,
                "acc_stderr,none": 0.024170840879341023
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.26595744680851063,
                "acc_stderr,none": 0.02635806569888059
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.3602941176470588,
                "acc_stderr,none": 0.029163128570670733
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.3192771084337349,
                "acc_stderr,none": 0.03629335329947859
            },
            "mmlu_social_sciences": {
                "acc,none": 0.23561910952226195,
                "acc_stderr,none": 0.00764020113188373,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.21052631578947367,
                "acc_stderr,none": 0.038351539543994194
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.18686868686868688,
                "acc_stderr,none": 0.027772533334218967
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.23834196891191708,
                "acc_stderr,none": 0.030748905363909878
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.2358974358974359,
                "acc_stderr,none": 0.02152596540740872
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.25210084033613445,
                "acc_stderr,none": 0.02820554503327772
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.23119266055045873,
                "acc_stderr,none": 0.018075750241633163
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.21374045801526717,
                "acc_stderr,none": 0.0359546161177469
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.25326797385620914,
                "acc_stderr,none": 0.01759348689536683
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.37272727272727274,
                "acc_stderr,none": 0.04631381319425464
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.18775510204081633,
                "acc_stderr,none": 0.02500025603954622
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.24875621890547264,
                "acc_stderr,none": 0.030567675938916714
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.2,
                "acc_stderr,none": 0.040201512610368445
            },
            "mmlu_stem": {
                "acc,none": 0.256581033935934,
                "acc_stderr,none": 0.007774222131918913,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.27,
                "acc_stderr,none": 0.044619604333847394
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.2074074074074074,
                "acc_stderr,none": 0.03502553170678316
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.2236842105263158,
                "acc_stderr,none": 0.033911609343436025
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.25,
                "acc_stderr,none": 0.03621034121889507
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.22,
                "acc_stderr,none": 0.041633319989322695
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.31,
                "acc_stderr,none": 0.04648231987117316
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.27,
                "acc_stderr,none": 0.0446196043338474
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.3627450980392157,
                "acc_stderr,none": 0.04784060704105654
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.32,
                "acc_stderr,none": 0.04688261722621504
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.3276595744680851,
                "acc_stderr,none": 0.030683020843231004
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.22758620689655173,
                "acc_stderr,none": 0.034939503801311826
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.23809523809523808,
                "acc_stderr,none": 0.021935878081184756
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.24516129032258063,
                "acc_stderr,none": 0.024472243840895514
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.2413793103448276,
                "acc_stderr,none": 0.030108330718011625
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.26,
                "acc_stderr,none": 0.0440844002276808
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.24814814814814815,
                "acc_stderr,none": 0.0263357394040558
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.2582781456953642,
                "acc_stderr,none": 0.035737053147634576
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.24537037037037038,
                "acc_stderr,none": 0.02934666509437295
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.22321428571428573,
                "acc_stderr,none": 0.039523019677025116
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.2477567298105683,
                "acc_stderr,none": 0.0036381371880230386,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.23230605738575982,
                "acc_stderr,none": 0.006157628254739183,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.27421950434502734,
                "acc_stderr,none": 0.007995123598653522,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.23561910952226195,
                "acc_stderr,none": 0.00764020113188373,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.256581033935934,
                "acc_stderr,none": 0.007774222131918913,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=EleutherAI/pythia-2.8b",
            "model_num_parameters": 2775208960,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "2a259cdd96a4beb1cdf467512e3904197345f6a9",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721404964.9847882,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "0"
        ],
        "eot_token_id": 0,
        "max_length": 2048
    },
    "pythia-2.8b_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.29351073555476886,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=EleutherAI/pythia-2.8b",
            "model_num_parameters": 2775208960,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "2a259cdd96a4beb1cdf467512e3904197345f6a9",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721390587.4397194,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "0"
        ],
        "eot_token_id": 0,
        "max_length": 2048
    },
    "pythia-410m_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.2337986041874377,
                "acc_stderr,none": 0.0035676665740106625,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.2405951115834219,
                "acc_stderr,none": 0.006230832837661335,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.25396825396825395,
                "acc_stderr,none": 0.03893259610604673
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.19393939393939394,
                "acc_stderr,none": 0.030874145136562083
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.2647058823529412,
                "acc_stderr,none": 0.030964517926923393
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.2616033755274262,
                "acc_stderr,none": 0.028609516716994934
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.2396694214876033,
                "acc_stderr,none": 0.03896878985070417
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.25925925925925924,
                "acc_stderr,none": 0.04236511258094634
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.2085889570552147,
                "acc_stderr,none": 0.03192193448934722
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.24277456647398843,
                "acc_stderr,none": 0.0230836585869842
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.23575418994413408,
                "acc_stderr,none": 0.014196375686290804
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.1864951768488746,
                "acc_stderr,none": 0.02212243977248077
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.22839506172839505,
                "acc_stderr,none": 0.023358211840626267
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.24902216427640156,
                "acc_stderr,none": 0.01104489226404077
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.30409356725146197,
                "acc_stderr,none": 0.03528211258245231
            },
            "mmlu_other": {
                "acc,none": 0.2481493401995494,
                "acc_stderr,none": 0.007737811523778621,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.2188679245283019,
                "acc_stderr,none": 0.0254478638251086
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.2138728323699422,
                "acc_stderr,none": 0.031265112061730424
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.19,
                "acc_stderr,none": 0.039427724440366234
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.31390134529147984,
                "acc_stderr,none": 0.03114679648297246
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.20388349514563106,
                "acc_stderr,none": 0.03989139859531772
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.2905982905982906,
                "acc_stderr,none": 0.029745048572674057
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.2541507024265645,
                "acc_stderr,none": 0.015569254692045759
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.2222222222222222,
                "acc_stderr,none": 0.023805186524888146
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.25177304964539005,
                "acc_stderr,none": 0.025892151156709405
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.1948529411764706,
                "acc_stderr,none": 0.024060599423487424
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.28313253012048195,
                "acc_stderr,none": 0.03507295431370518
            },
            "mmlu_social_sciences": {
                "acc,none": 0.21774455638609036,
                "acc_stderr,none": 0.007437846699903758,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.22807017543859648,
                "acc_stderr,none": 0.03947152782669415
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.18181818181818182,
                "acc_stderr,none": 0.027479603010538804
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.20207253886010362,
                "acc_stderr,none": 0.02897908979429673
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.2076923076923077,
                "acc_stderr,none": 0.020567539567246794
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.21008403361344538,
                "acc_stderr,none": 0.026461398717471874
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.1908256880733945,
                "acc_stderr,none": 0.016847676400091088
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.2595419847328244,
                "acc_stderr,none": 0.03844876139785271
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.25,
                "acc_stderr,none": 0.01751781884501444
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.21818181818181817,
                "acc_stderr,none": 0.03955932861795833
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.18775510204081633,
                "acc_stderr,none": 0.02500025603954622
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.24378109452736318,
                "acc_stderr,none": 0.030360490154014652
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.28,
                "acc_stderr,none": 0.045126085985421276
            },
            "mmlu_stem": {
                "acc,none": 0.22518236600063432,
                "acc_stderr,none": 0.007427732578470911,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.26,
                "acc_stderr,none": 0.0440844002276808
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.18518518518518517,
                "acc_stderr,none": 0.03355677216313142
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.19736842105263158,
                "acc_stderr,none": 0.03238981601699397
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2569444444444444,
                "acc_stderr,none": 0.03653946969442099
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.28,
                "acc_stderr,none": 0.045126085985421276
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.23,
                "acc_stderr,none": 0.04229525846816505
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.24,
                "acc_stderr,none": 0.04292346959909284
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.23529411764705882,
                "acc_stderr,none": 0.04220773659171452
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.24,
                "acc_stderr,none": 0.042923469599092816
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.25957446808510637,
                "acc_stderr,none": 0.02865917937429232
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2413793103448276,
                "acc_stderr,none": 0.03565998174135302
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.19576719576719576,
                "acc_stderr,none": 0.020435730971541787
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.18064516129032257,
                "acc_stderr,none": 0.021886178567172558
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.16748768472906403,
                "acc_stderr,none": 0.02627308604753542
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.27,
                "acc_stderr,none": 0.0446196043338474
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.2518518518518518,
                "acc_stderr,none": 0.02646611753895991
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.2251655629139073,
                "acc_stderr,none": 0.03410435282008937
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.19444444444444445,
                "acc_stderr,none": 0.026991454502036733
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.3392857142857143,
                "acc_stderr,none": 0.04493949068613539
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.2337986041874377,
                "acc_stderr,none": 0.0035676665740106625,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.2405951115834219,
                "acc_stderr,none": 0.006230832837661335,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.2481493401995494,
                "acc_stderr,none": 0.007737811523778621,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.21774455638609036,
                "acc_stderr,none": 0.007437846699903758,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.22518236600063432,
                "acc_stderr,none": 0.007427732578470911,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=EleutherAI/pythia-410m",
            "model_num_parameters": 405334016,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "9879c9b5f8bea9051dcb0e68dff21493d67e9d4f",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721401592.52648,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "0"
        ],
        "eot_token_id": 0,
        "max_length": 2048
    },
    "pythia-410m_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.29070446976529163,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=EleutherAI/pythia-410m",
            "model_num_parameters": 405334016,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "9879c9b5f8bea9051dcb0e68dff21493d67e9d4f",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721389947.211334,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "0"
        ],
        "eot_token_id": 0,
        "max_length": 2048
    },
    "pythia-6.9b_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.2550206523287281,
                "acc_stderr,none": 0.003676331633411023,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.261211477151966,
                "acc_stderr,none": 0.006404035695905985,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2857142857142857,
                "acc_stderr,none": 0.04040610178208841
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.22424242424242424,
                "acc_stderr,none": 0.032568666616811015
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.28431372549019607,
                "acc_stderr,none": 0.031660096793998116
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.2489451476793249,
                "acc_stderr,none": 0.028146970599422644
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.33884297520661155,
                "acc_stderr,none": 0.04320767807536669
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.26851851851851855,
                "acc_stderr,none": 0.04284467968052191
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.22699386503067484,
                "acc_stderr,none": 0.03291099578615769
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.2543352601156069,
                "acc_stderr,none": 0.02344582627654555
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.24804469273743016,
                "acc_stderr,none": 0.014444157808261438
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.2861736334405145,
                "acc_stderr,none": 0.025670259242188943
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.28703703703703703,
                "acc_stderr,none": 0.025171041915309684
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.2516297262059974,
                "acc_stderr,none": 0.011083276280441902
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.3157894736842105,
                "acc_stderr,none": 0.035650796707083106
            },
            "mmlu_other": {
                "acc,none": 0.24589636305117477,
                "acc_stderr,none": 0.00771762663417654,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.26,
                "acc_stderr,none": 0.0440844002276808
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.1811320754716981,
                "acc_stderr,none": 0.0237029635267578
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.21965317919075145,
                "acc_stderr,none": 0.031568093627031744
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.32,
                "acc_stderr,none": 0.046882617226215034
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.23766816143497757,
                "acc_stderr,none": 0.028568079464714277
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.27184466019417475,
                "acc_stderr,none": 0.044052680241409216
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.2777777777777778,
                "acc_stderr,none": 0.029343114798094486
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.27330779054916987,
                "acc_stderr,none": 0.015936681062628556
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.25163398692810457,
                "acc_stderr,none": 0.024848018263875195
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.24113475177304963,
                "acc_stderr,none": 0.02551873104953777
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.1948529411764706,
                "acc_stderr,none": 0.024060599423487424
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.2469879518072289,
                "acc_stderr,none": 0.03357351982064536
            },
            "mmlu_social_sciences": {
                "acc,none": 0.2447188820279493,
                "acc_stderr,none": 0.007757453907823362,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.2543859649122807,
                "acc_stderr,none": 0.040969851398436716
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.2676767676767677,
                "acc_stderr,none": 0.03154449888270285
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.27461139896373055,
                "acc_stderr,none": 0.032210245080411516
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.24871794871794872,
                "acc_stderr,none": 0.021916957709213796
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.22268907563025211,
                "acc_stderr,none": 0.02702543349888237
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.24403669724770644,
                "acc_stderr,none": 0.0184152863514164
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.22137404580152673,
                "acc_stderr,none": 0.0364129708131373
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.2581699346405229,
                "acc_stderr,none": 0.017704531653250078
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.2727272727272727,
                "acc_stderr,none": 0.04265792110940588
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.21224489795918366,
                "acc_stderr,none": 0.026176967197866767
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.22885572139303484,
                "acc_stderr,none": 0.029705284056772443
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.2,
                "acc_stderr,none": 0.04020151261036845
            },
            "mmlu_stem": {
                "acc,none": 0.26482714874722485,
                "acc_stderr,none": 0.007848195982273504,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.31,
                "acc_stderr,none": 0.04648231987117316
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.3333333333333333,
                "acc_stderr,none": 0.04072314811876837
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.3157894736842105,
                "acc_stderr,none": 0.0378272898086547
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2777777777777778,
                "acc_stderr,none": 0.03745554791462457
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.15,
                "acc_stderr,none": 0.035887028128263714
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.28,
                "acc_stderr,none": 0.04512608598542128
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.27,
                "acc_stderr,none": 0.044619604333847394
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.19607843137254902,
                "acc_stderr,none": 0.039505818611799616
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.2127659574468085,
                "acc_stderr,none": 0.026754391348039776
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.31724137931034485,
                "acc_stderr,none": 0.038783523721386215
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.2619047619047619,
                "acc_stderr,none": 0.022644212615525218
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.25483870967741934,
                "acc_stderr,none": 0.0247901184593322
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.29064039408866993,
                "acc_stderr,none": 0.0319474007226554
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.34,
                "acc_stderr,none": 0.047609522856952365
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.26296296296296295,
                "acc_stderr,none": 0.026842057873833706
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.26490066225165565,
                "acc_stderr,none": 0.03603038545360383
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.25,
                "acc_stderr,none": 0.029531221160930918
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.25,
                "acc_stderr,none": 0.04109974682633932
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.2550206523287281,
                "acc_stderr,none": 0.003676331633411023,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.261211477151966,
                "acc_stderr,none": 0.006404035695905985,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.24589636305117477,
                "acc_stderr,none": 0.00771762663417654,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.2447188820279493,
                "acc_stderr,none": 0.007757453907823362,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.26482714874722485,
                "acc_stderr,none": 0.007848195982273504,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=EleutherAI/pythia-6.9b",
            "model_num_parameters": 6857302016,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "f271943e880e60c0c715fd10e4dc74ec4e31eb44",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721407872.2707882,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "0"
        ],
        "eot_token_id": 0,
        "max_length": 2048
    },
    "pythia-6.9b_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.28936477294698365,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=EleutherAI/pythia-6.9b",
            "model_num_parameters": 6857302016,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "f271943e880e60c0c715fd10e4dc74ec4e31eb44",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721390672.6254082,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "0"
        ],
        "eot_token_id": 0,
        "max_length": 2048
    },
    "pythia-70m_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.22959692351516878,
                "acc_stderr,none": 0.003543630146296216,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.2414452709883103,
                "acc_stderr,none": 0.006237484816648882,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2619047619047619,
                "acc_stderr,none": 0.03932537680392872
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.21818181818181817,
                "acc_stderr,none": 0.03225078108306289
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.25,
                "acc_stderr,none": 0.03039153369274154
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.270042194092827,
                "acc_stderr,none": 0.028900721906293426
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.2396694214876033,
                "acc_stderr,none": 0.03896878985070417
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.25925925925925924,
                "acc_stderr,none": 0.04236511258094634
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.22085889570552147,
                "acc_stderr,none": 0.032591773927421776
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.2514450867052023,
                "acc_stderr,none": 0.023357365785874037
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.23798882681564246,
                "acc_stderr,none": 0.014242630070574885
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.1864951768488746,
                "acc_stderr,none": 0.02212243977248077
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.21296296296296297,
                "acc_stderr,none": 0.022779719088733396
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.2457627118644068,
                "acc_stderr,none": 0.01099615663514269
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.3216374269005848,
                "acc_stderr,none": 0.03582529442573122
            },
            "mmlu_other": {
                "acc,none": 0.23978113936272932,
                "acc_stderr,none": 0.007642603193393712,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.21509433962264152,
                "acc_stderr,none": 0.025288394502891377
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.2138728323699422,
                "acc_stderr,none": 0.03126511206173043
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.18,
                "acc_stderr,none": 0.038612291966536955
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.31390134529147984,
                "acc_stderr,none": 0.03114679648297246
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.17475728155339806,
                "acc_stderr,none": 0.03760178006026621
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.2905982905982906,
                "acc_stderr,none": 0.029745048572674057
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.23627075351213284,
                "acc_stderr,none": 0.01519047371703751
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.22549019607843138,
                "acc_stderr,none": 0.023929155517351284
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.23404255319148937,
                "acc_stderr,none": 0.025257861359432407
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.18382352941176472,
                "acc_stderr,none": 0.02352924218519311
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.28313253012048195,
                "acc_stderr,none": 0.03507295431370518
            },
            "mmlu_social_sciences": {
                "acc,none": 0.2170945726356841,
                "acc_stderr,none": 0.007427764024212041,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.23684210526315788,
                "acc_stderr,none": 0.039994238792813386
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.17676767676767677,
                "acc_stderr,none": 0.027178752639044915
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.19689119170984457,
                "acc_stderr,none": 0.02869787397186069
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.20256410256410257,
                "acc_stderr,none": 0.020377660970371397
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.21008403361344538,
                "acc_stderr,none": 0.026461398717471874
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.1908256880733945,
                "acc_stderr,none": 0.016847676400091084
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.26717557251908397,
                "acc_stderr,none": 0.038808483010823965
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.25,
                "acc_stderr,none": 0.01751781884501444
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.21818181818181817,
                "acc_stderr,none": 0.03955932861795833
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.18775510204081633,
                "acc_stderr,none": 0.02500025603954622
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.24378109452736318,
                "acc_stderr,none": 0.030360490154014652
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.28,
                "acc_stderr,none": 0.045126085985421276
            },
            "mmlu_stem": {
                "acc,none": 0.21408182683158897,
                "acc_stderr,none": 0.007290857329985196,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.22,
                "acc_stderr,none": 0.04163331998932269
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.18518518518518517,
                "acc_stderr,none": 0.03355677216313142
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.17763157894736842,
                "acc_stderr,none": 0.031103182383123398
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2638888888888889,
                "acc_stderr,none": 0.03685651095897532
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.2,
                "acc_stderr,none": 0.040201512610368445
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.26,
                "acc_stderr,none": 0.044084400227680794
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.21568627450980393,
                "acc_stderr,none": 0.040925639582376556
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.29,
                "acc_stderr,none": 0.045604802157206845
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.26382978723404255,
                "acc_stderr,none": 0.02880998985410298
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2413793103448276,
                "acc_stderr,none": 0.03565998174135302
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.20634920634920634,
                "acc_stderr,none": 0.02084229093011466
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.18064516129032257,
                "acc_stderr,none": 0.021886178567172555
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.1625615763546798,
                "acc_stderr,none": 0.02596030006460558
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.26,
                "acc_stderr,none": 0.04408440022768079
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.2111111111111111,
                "acc_stderr,none": 0.02488211685765508
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.1986754966887417,
                "acc_stderr,none": 0.032578473844367746
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.1527777777777778,
                "acc_stderr,none": 0.02453632602613422
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.3125,
                "acc_stderr,none": 0.043994650575715215
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.22959692351516878,
                "acc_stderr,none": 0.003543630146296216,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.2414452709883103,
                "acc_stderr,none": 0.006237484816648882,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.23978113936272932,
                "acc_stderr,none": 0.007642603193393712,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.2170945726356841,
                "acc_stderr,none": 0.007427764024212041,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.21408182683158897,
                "acc_stderr,none": 0.007290857329985196,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=EleutherAI/pythia-70m",
            "model_num_parameters": 70426624,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "a39f36b100fe8a5377810d56c3f4789b9c53ac42",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721400502.086123,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "0"
        ],
        "eot_token_id": 0,
        "max_length": 2048
    },
    "pythia-70m_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.3140600501289459,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=EleutherAI/pythia-70m",
            "model_num_parameters": 70426624,
            "model_dtype": "torch.float16",
            "model_revision": "main",
            "model_sha": "a39f36b100fe8a5377810d56c3f4789b9c53ac42",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721389887.452697,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "0"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "0"
        ],
        "eot_token_id": 0,
        "max_length": 2048
    },
    "mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.6213502350092579,
                "acc_stderr,none": 0.0038101244785158327,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.5496280552603613,
                "acc_stderr,none": 0.006647592896214338,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.42063492063492064,
                "acc_stderr,none": 0.04415438226743744
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.7272727272727273,
                "acc_stderr,none": 0.03477691162163659
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.803921568627451,
                "acc_stderr,none": 0.02786594228663933
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.810126582278481,
                "acc_stderr,none": 0.025530100460233508
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.7851239669421488,
                "acc_stderr,none": 0.037494924487096966
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.7222222222222222,
                "acc_stderr,none": 0.043300437496507416
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.7484662576687117,
                "acc_stderr,none": 0.034089978868575295
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.6965317919075145,
                "acc_stderr,none": 0.024752411960917205
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.23798882681564246,
                "acc_stderr,none": 0.014242630070574885
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.7170418006430869,
                "acc_stderr,none": 0.025583062489984827
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.7283950617283951,
                "acc_stderr,none": 0.024748624490537375
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.4634941329856584,
                "acc_stderr,none": 0.012736153390214963
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.8070175438596491,
                "acc_stderr,none": 0.030267457554898458
            },
            "mmlu_other": {
                "acc,none": 0.7119407788863856,
                "acc_stderr,none": 0.00775946669132189,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.56,
                "acc_stderr,none": 0.04988876515698589
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.7509433962264151,
                "acc_stderr,none": 0.026616482980501715
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.6184971098265896,
                "acc_stderr,none": 0.037038511930995194
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.35,
                "acc_stderr,none": 0.047937248544110196
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.672645739910314,
                "acc_stderr,none": 0.03149384670994131
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.8640776699029126,
                "acc_stderr,none": 0.033932957297610096
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.8760683760683761,
                "acc_stderr,none": 0.02158649400128138
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.83,
                "acc_stderr,none": 0.0377525168068637
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.8263090676883781,
                "acc_stderr,none": 0.01354741565866226
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.738562091503268,
                "acc_stderr,none": 0.02516099821429246
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.4716312056737589,
                "acc_stderr,none": 0.029779450957303055
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.6948529411764706,
                "acc_stderr,none": 0.0279715413701706
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.5602409638554217,
                "acc_stderr,none": 0.03864139923699121
            },
            "mmlu_social_sciences": {
                "acc,none": 0.7312317192070198,
                "acc_stderr,none": 0.007800017894983416,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.39473684210526316,
                "acc_stderr,none": 0.04598188057816542
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.7828282828282829,
                "acc_stderr,none": 0.02937661648494564
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.8652849740932642,
                "acc_stderr,none": 0.024639789097709433
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.6205128205128205,
                "acc_stderr,none": 0.02460362692409741
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.7016806722689075,
                "acc_stderr,none": 0.029719142876342863
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.8238532110091743,
                "acc_stderr,none": 0.01633288239343138
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.7557251908396947,
                "acc_stderr,none": 0.03768335959728745
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.6911764705882353,
                "acc_stderr,none": 0.018690850273595284
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.6636363636363637,
                "acc_stderr,none": 0.04525393596302506
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.7387755102040816,
                "acc_stderr,none": 0.028123429335142797
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.8208955223880597,
                "acc_stderr,none": 0.027113286753111837
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.84,
                "acc_stderr,none": 0.03684529491774709
            },
            "mmlu_stem": {
                "acc,none": 0.5318744053282588,
                "acc_stderr,none": 0.008551973564436108,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.32,
                "acc_stderr,none": 0.046882617226215034
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.6888888888888889,
                "acc_stderr,none": 0.03999262876617723
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.6710526315789473,
                "acc_stderr,none": 0.03823428969926604
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.7777777777777778,
                "acc_stderr,none": 0.03476590104304134
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.44,
                "acc_stderr,none": 0.04988876515698589
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.48,
                "acc_stderr,none": 0.050211673156867795
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.38,
                "acc_stderr,none": 0.04878317312145633
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.37254901960784315,
                "acc_stderr,none": 0.048108401480826325
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.77,
                "acc_stderr,none": 0.04229525846816506
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.5276595744680851,
                "acc_stderr,none": 0.03263597118409769
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.6,
                "acc_stderr,none": 0.040824829046386284
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.42857142857142855,
                "acc_stderr,none": 0.025487187147859372
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.7516129032258064,
                "acc_stderr,none": 0.024580028921481
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.5270935960591133,
                "acc_stderr,none": 0.03512819077876106
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.65,
                "acc_stderr,none": 0.047937248544110196
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.3925925925925926,
                "acc_stderr,none": 0.02977384701253297
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.4105960264900662,
                "acc_stderr,none": 0.04016689594849927
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.4722222222222222,
                "acc_stderr,none": 0.03404705328653881
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.4017857142857143,
                "acc_stderr,none": 0.04653333146973647
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.6213502350092579,
                "acc_stderr,none": 0.0038101244785158327,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.5496280552603613,
                "acc_stderr,none": 0.006647592896214338,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.7119407788863856,
                "acc_stderr,none": 0.00775946669132189,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.7312317192070198,
                "acc_stderr,none": 0.007800017894983416,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.5318744053282588,
                "acc_stderr,none": 0.008551973564436108,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=meta-llama/Meta-Llama-3-8B",
            "model_num_parameters": 8030261248,
            "model_dtype": "torch.bfloat16",
            "model_revision": "main",
            "model_sha": "62bd457b6fe961a42a631306577e622c83876cb6",
            "batch_size": "auto",
            "batch_sizes": [
                4
            ],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721045678.0569787,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|end_of_text|>",
            "128001"
        ],
        "tokenizer_eos_token": [
            "<|end_of_text|>",
            "128001"
        ],
        "tokenizer_bos_token": [
            "<|begin_of_text|>",
            "128000"
        ],
        "eot_token_id": 128001,
        "max_length": 8192
    },
    "tinyHellaswag": {
        "results": {
            "tinyHellaswag": {
                "alias": "tinyHellaswag",
                "acc_norm,none": 0.3879128610374235,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyHellaswag": []
        },
        "configs": {
            "tinyHellaswag": {
                "task": "tinyHellaswag",
                "dataset_path": "tinyBenchmarks/tinyHellaswag",
                "training_split": "train",
                "validation_split": "validation",
                "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _process_doc(doc):\n        ctx = doc[\"ctx_a\"] + \" \" + doc[\"ctx_b\"].capitalize()\n        out_doc = {\n            \"query\": preprocess(doc[\"activity_label\"] + \": \" + ctx),\n            \"choices\": [preprocess(ending) for ending in doc[\"endings\"]],\n            \"gold\": int(doc[\"label\"]),\n        }\n        return out_doc\n\n    return dataset.map(_process_doc)\n",
                "doc_to_text": "{{query}}",
                "doc_to_target": "{{label}}",
                "doc_to_choice": "choices",
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "num_fewshot": 10,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_hellaswag(items: List[float], benchmark: str = \"hellaswag\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyHellaswag": 0.0
        },
        "n-shot": {
            "tinyHellaswag": 10
        },
        "higher_is_better": {
            "tinyHellaswag": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyHellaswag": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=openai-community/gpt2",
            "model_num_parameters": 124439808,
            "model_dtype": "torch.float32",
            "model_revision": "main",
            "model_sha": "607a30d783dfa663caf39e06633721c8d4cfcd7e",
            "batch_size": "auto",
            "batch_sizes": [
                32
            ],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721044695.3949373,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "50256"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "50256"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "50256"
        ],
        "eot_token_id": 50256,
        "max_length": 1024
    },
    "tinyMmlu": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.6427457763232849,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=meta-llama/Meta-Llama-3-8B",
            "model_num_parameters": 8030261248,
            "model_dtype": "torch.bfloat16",
            "model_revision": "main",
            "model_sha": "62bd457b6fe961a42a631306577e622c83876cb6",
            "batch_size": "auto",
            "batch_sizes": [
                2
            ],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721045444.8915818,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|end_of_text|>",
            "128001"
        ],
        "tokenizer_eos_token": [
            "<|end_of_text|>",
            "128001"
        ],
        "tokenizer_bos_token": [
            "<|begin_of_text|>",
            "128000"
        ],
        "eot_token_id": 128001,
        "max_length": 8192
    },
    "gpt2-large_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.23123486682808717,
                "acc_stderr,none": 0.0035533217661025715,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.24484590860786398,
                "acc_stderr,none": 0.00626681233388005,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2777777777777778,
                "acc_stderr,none": 0.04006168083848877
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.21818181818181817,
                "acc_stderr,none": 0.03225078108306289
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.24509803921568626,
                "acc_stderr,none": 0.030190282453501967
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.270042194092827,
                "acc_stderr,none": 0.028900721906293426
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.2396694214876033,
                "acc_stderr,none": 0.03896878985070417
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.25925925925925924,
                "acc_stderr,none": 0.04236511258094634
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.22085889570552147,
                "acc_stderr,none": 0.032591773927421776
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.24855491329479767,
                "acc_stderr,none": 0.023267528432100174
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.2547486033519553,
                "acc_stderr,none": 0.014572650383409162
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.1864951768488746,
                "acc_stderr,none": 0.02212243977248077
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.21604938271604937,
                "acc_stderr,none": 0.022899162918445813
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.2457627118644068,
                "acc_stderr,none": 0.01099615663514269
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.3216374269005848,
                "acc_stderr,none": 0.035825294425731215
            },
            "mmlu_other": {
                "acc,none": 0.24106855487608625,
                "acc_stderr,none": 0.0076594135312773514,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.21132075471698114,
                "acc_stderr,none": 0.025125766484827856
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.21965317919075145,
                "acc_stderr,none": 0.031568093627031744
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.23,
                "acc_stderr,none": 0.04229525846816507
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.31390134529147984,
                "acc_stderr,none": 0.03114679648297246
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.18446601941747573,
                "acc_stderr,none": 0.03840423627288276
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.2905982905982906,
                "acc_stderr,none": 0.029745048572674057
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.2388250319284802,
                "acc_stderr,none": 0.015246803197398677
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.22875816993464052,
                "acc_stderr,none": 0.024051029739912258
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.23049645390070922,
                "acc_stderr,none": 0.025123739226872405
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.17647058823529413,
                "acc_stderr,none": 0.02315746830855936
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.2710843373493976,
                "acc_stderr,none": 0.034605799075530276
            },
            "mmlu_social_sciences": {
                "acc,none": 0.21611959701007474,
                "acc_stderr,none": 0.007418731864179047,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.21929824561403508,
                "acc_stderr,none": 0.0389243110651875
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.16666666666666666,
                "acc_stderr,none": 0.026552207828215293
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.19689119170984457,
                "acc_stderr,none": 0.02869787397186069
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.2,
                "acc_stderr,none": 0.020280805062535726
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.21008403361344538,
                "acc_stderr,none": 0.026461398717471874
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.2036697247706422,
                "acc_stderr,none": 0.017266742087630804
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.25190839694656486,
                "acc_stderr,none": 0.038073871163060866
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.24836601307189543,
                "acc_stderr,none": 0.017479487001364764
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.22727272727272727,
                "acc_stderr,none": 0.040139645540727735
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.1836734693877551,
                "acc_stderr,none": 0.024789071332007643
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.23880597014925373,
                "acc_stderr,none": 0.030147775935409227
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.27,
                "acc_stderr,none": 0.0446196043338474
            },
            "mmlu_stem": {
                "acc,none": 0.2159847764034253,
                "acc_stderr,none": 0.007319970274657318,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.22,
                "acc_stderr,none": 0.04163331998932269
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.18518518518518517,
                "acc_stderr,none": 0.03355677216313142
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.17763157894736842,
                "acc_stderr,none": 0.031103182383123398
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2638888888888889,
                "acc_stderr,none": 0.03685651095897532
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.19,
                "acc_stderr,none": 0.03942772444036623
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.26,
                "acc_stderr,none": 0.044084400227680794
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.2,
                "acc_stderr,none": 0.040201512610368445
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.21568627450980393,
                "acc_stderr,none": 0.040925639582376556
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.23,
                "acc_stderr,none": 0.042295258468165065
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.2680851063829787,
                "acc_stderr,none": 0.02895734278834235
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2413793103448276,
                "acc_stderr,none": 0.03565998174135302
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.21428571428571427,
                "acc_stderr,none": 0.021132859182754423
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.17419354838709677,
                "acc_stderr,none": 0.021576248184514587
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.15763546798029557,
                "acc_stderr,none": 0.0256390141311724
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.28,
                "acc_stderr,none": 0.04512608598542128
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.22592592592592592,
                "acc_stderr,none": 0.025497532639609546
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.1986754966887417,
                "acc_stderr,none": 0.032578473844367746
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.18981481481481483,
                "acc_stderr,none": 0.026744714834691936
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.30357142857142855,
                "acc_stderr,none": 0.04364226155841044
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.23123486682808717,
                "acc_stderr,none": 0.0035533217661025715,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.24484590860786398,
                "acc_stderr,none": 0.00626681233388005,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.24106855487608625,
                "acc_stderr,none": 0.0076594135312773514,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.21611959701007474,
                "acc_stderr,none": 0.007418731864179047,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.2159847764034253,
                "acc_stderr,none": 0.007319970274657318,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=openai-community/gpt2-large",
            "model_num_parameters": 774030080,
            "model_dtype": "torch.float32",
            "model_revision": "main",
            "model_sha": "32b71b12589c2f8d625668d2335a01cac3249519",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721391770.021687,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "50256"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "50256"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "50256"
        ],
        "eot_token_id": 50256,
        "max_length": 1024
    },
    "gpt2-large_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.3162257837262001,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=openai-community/gpt2-large",
            "model_num_parameters": 774030080,
            "model_dtype": "torch.float32",
            "model_revision": "main",
            "model_sha": "32b71b12589c2f8d625668d2335a01cac3249519",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721389689.827013,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "50256"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "50256"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "50256"
        ],
        "eot_token_id": 50256,
        "max_length": 1024
    },
    "gpt2-medium_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.23180458624127617,
                "acc_stderr,none": 0.0035550979867931135,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.2501594048884166,
                "acc_stderr,none": 0.006310589741053362,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.29365079365079366,
                "acc_stderr,none": 0.040735243221471276
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.21818181818181817,
                "acc_stderr,none": 0.03225078108306289
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.24509803921568626,
                "acc_stderr,none": 0.030190282453501967
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.26582278481012656,
                "acc_stderr,none": 0.028756799629658335
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.2396694214876033,
                "acc_stderr,none": 0.03896878985070417
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.25,
                "acc_stderr,none": 0.04186091791394607
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.22085889570552147,
                "acc_stderr,none": 0.032591773927421776
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.2630057803468208,
                "acc_stderr,none": 0.02370309952525817
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.2759776536312849,
                "acc_stderr,none": 0.014950103002475353
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.1864951768488746,
                "acc_stderr,none": 0.02212243977248077
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.2222222222222222,
                "acc_stderr,none": 0.023132376234543325
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.24641460234680573,
                "acc_stderr,none": 0.011005971399927246
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.30994152046783624,
                "acc_stderr,none": 0.03546976959393161
            },
            "mmlu_other": {
                "acc,none": 0.23881557772771161,
                "acc_stderr,none": 0.007629415660123214,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.2037735849056604,
                "acc_stderr,none": 0.024790784501775416
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.20809248554913296,
                "acc_stderr,none": 0.03095289021774988
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.19,
                "acc_stderr,none": 0.03942772444036623
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.3183856502242152,
                "acc_stderr,none": 0.03126580522513713
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.17475728155339806,
                "acc_stderr,none": 0.03760178006026621
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.2948717948717949,
                "acc_stderr,none": 0.029872577708891172
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.23499361430395913,
                "acc_stderr,none": 0.01516202415227843
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.20588235294117646,
                "acc_stderr,none": 0.023152722439402307
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.24113475177304963,
                "acc_stderr,none": 0.025518731049537776
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.1948529411764706,
                "acc_stderr,none": 0.02406059942348742
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.28313253012048195,
                "acc_stderr,none": 0.03507295431370518
            },
            "mmlu_social_sciences": {
                "acc,none": 0.21774455638609036,
                "acc_stderr,none": 0.007436423270940342,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.22807017543859648,
                "acc_stderr,none": 0.03947152782669415
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.17676767676767677,
                "acc_stderr,none": 0.027178752639044915
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.19689119170984457,
                "acc_stderr,none": 0.02869787397186069
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.21025641025641026,
                "acc_stderr,none": 0.020660597485026935
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.20168067226890757,
                "acc_stderr,none": 0.026064313406304534
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.1908256880733945,
                "acc_stderr,none": 0.01684767640009109
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.24427480916030533,
                "acc_stderr,none": 0.03768335959728744
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.2581699346405229,
                "acc_stderr,none": 0.017704531653250075
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.21818181818181817,
                "acc_stderr,none": 0.03955932861795833
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.19183673469387755,
                "acc_stderr,none": 0.025206963154225392
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.24378109452736318,
                "acc_stderr,none": 0.030360490154014652
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.27,
                "acc_stderr,none": 0.0446196043338474
            },
            "mmlu_stem": {
                "acc,none": 0.21122740247383445,
                "acc_stderr,none": 0.007266233273467983,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.18,
                "acc_stderr,none": 0.03861229196653695
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.1925925925925926,
                "acc_stderr,none": 0.03406542058502652
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.18421052631578946,
                "acc_stderr,none": 0.0315469804508223
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2569444444444444,
                "acc_stderr,none": 0.03653946969442099
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.17,
                "acc_stderr,none": 0.0377525168068637
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.23,
                "acc_stderr,none": 0.04229525846816505
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.24,
                "acc_stderr,none": 0.04292346959909283
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.18627450980392157,
                "acc_stderr,none": 0.03873958714149352
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.28,
                "acc_stderr,none": 0.045126085985421276
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.26382978723404255,
                "acc_stderr,none": 0.02880998985410298
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2482758620689655,
                "acc_stderr,none": 0.0360010569272777
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.21164021164021163,
                "acc_stderr,none": 0.021037331505262886
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.1774193548387097,
                "acc_stderr,none": 0.021732540689329272
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.1921182266009852,
                "acc_stderr,none": 0.027719315709614785
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.27,
                "acc_stderr,none": 0.0446196043338474
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.21851851851851853,
                "acc_stderr,none": 0.025195752251823786
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.19205298013245034,
                "acc_stderr,none": 0.03216298420593613
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.16203703703703703,
                "acc_stderr,none": 0.02513045365226846
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.21428571428571427,
                "acc_stderr,none": 0.03894641120044792
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.23180458624127617,
                "acc_stderr,none": 0.0035550979867931135,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.2501594048884166,
                "acc_stderr,none": 0.006310589741053362,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.23881557772771161,
                "acc_stderr,none": 0.007629415660123214,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.21774455638609036,
                "acc_stderr,none": 0.007436423270940342,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.21122740247383445,
                "acc_stderr,none": 0.007266233273467983,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=openai-community/gpt2-medium",
            "model_num_parameters": 354823168,
            "model_dtype": "torch.float32",
            "model_revision": "main",
            "model_sha": "6dcaa7a952f72f9298047fd5137cd6e4f05f41da",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721392746.6287653,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "50256"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "50256"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "50256"
        ],
        "eot_token_id": 50256,
        "max_length": 1024
    },
    "gpt2-medium_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.2927750508260582,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=openai-community/gpt2-medium",
            "model_num_parameters": 354823168,
            "model_dtype": "torch.float32",
            "model_revision": "main",
            "model_sha": "6dcaa7a952f72f9298047fd5137cd6e4f05f41da",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721389727.0640602,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "50256"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "50256"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "50256"
        ],
        "eot_token_id": 50256,
        "max_length": 1024
    },
    "gpt2-xl_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.25573280159521433,
                "acc_stderr,none": 0.003679818469152143,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.24973432518597238,
                "acc_stderr,none": 0.006309482198066187,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2698412698412698,
                "acc_stderr,none": 0.03970158273235172
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.23030303030303031,
                "acc_stderr,none": 0.032876667586034886
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.22549019607843138,
                "acc_stderr,none": 0.02933116229425172
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.25738396624472576,
                "acc_stderr,none": 0.02845882099146029
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.2644628099173554,
                "acc_stderr,none": 0.040261875275912046
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.2777777777777778,
                "acc_stderr,none": 0.043300437496507437
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.31901840490797545,
                "acc_stderr,none": 0.03661997551073836
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.26011560693641617,
                "acc_stderr,none": 0.023618678310069353
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.23798882681564246,
                "acc_stderr,none": 0.014242630070574875
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.21864951768488747,
                "acc_stderr,none": 0.023475581417861102
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.2222222222222222,
                "acc_stderr,none": 0.023132376234543322
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.2503259452411995,
                "acc_stderr,none": 0.01106415102716544
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.3216374269005848,
                "acc_stderr,none": 0.03582529442573122
            },
            "mmlu_other": {
                "acc,none": 0.2574831026713872,
                "acc_stderr,none": 0.007845140895775557,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.2188679245283019,
                "acc_stderr,none": 0.025447863825108594
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.24277456647398843,
                "acc_stderr,none": 0.0326926380614177
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.29,
                "acc_stderr,none": 0.04560480215720683
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.2914798206278027,
                "acc_stderr,none": 0.030500283176545902
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.24271844660194175,
                "acc_stderr,none": 0.04245022486384493
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.2863247863247863,
                "acc_stderr,none": 0.02961432369045666
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.26181353767560667,
                "acc_stderr,none": 0.015720838678445256
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.25163398692810457,
                "acc_stderr,none": 0.0248480182638752
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.24113475177304963,
                "acc_stderr,none": 0.025518731049537783
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.20588235294117646,
                "acc_stderr,none": 0.024562204314142314
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.2891566265060241,
                "acc_stderr,none": 0.03529486801511115
            },
            "mmlu_social_sciences": {
                "acc,none": 0.25706857328566785,
                "acc_stderr,none": 0.007866513295675203,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.21929824561403508,
                "acc_stderr,none": 0.03892431106518752
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.2676767676767677,
                "acc_stderr,none": 0.031544498882702866
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.25906735751295334,
                "acc_stderr,none": 0.031618779179354094
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.3384615384615385,
                "acc_stderr,none": 0.02399150050031304
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.2689075630252101,
                "acc_stderr,none": 0.028801392193631276
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.23669724770642203,
                "acc_stderr,none": 0.01822407811729907
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.25190839694656486,
                "acc_stderr,none": 0.038073871163060866
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.2581699346405229,
                "acc_stderr,none": 0.017704531653250075
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.20909090909090908,
                "acc_stderr,none": 0.03895091015724138
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.20816326530612245,
                "acc_stderr,none": 0.025991117672813292
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.22885572139303484,
                "acc_stderr,none": 0.02970528405677243
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.27,
                "acc_stderr,none": 0.0446196043338474
            },
            "mmlu_stem": {
                "acc,none": 0.26165556612749763,
                "acc_stderr,none": 0.007825131313123073,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.19,
                "acc_stderr,none": 0.03942772444036623
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.3037037037037037,
                "acc_stderr,none": 0.039725528847851355
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.21710526315789475,
                "acc_stderr,none": 0.033550453048829226
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2847222222222222,
                "acc_stderr,none": 0.03773809990686934
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.27,
                "acc_stderr,none": 0.04461960433384739
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.23,
                "acc_stderr,none": 0.04229525846816505
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.31,
                "acc_stderr,none": 0.04648231987117316
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.23529411764705882,
                "acc_stderr,none": 0.04220773659171451
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.24,
                "acc_stderr,none": 0.042923469599092816
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.23404255319148937,
                "acc_stderr,none": 0.02767845257821241
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2620689655172414,
                "acc_stderr,none": 0.036646663372252565
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.26455026455026454,
                "acc_stderr,none": 0.022717467897708617
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.26129032258064516,
                "acc_stderr,none": 0.024993053397764822
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.24630541871921183,
                "acc_stderr,none": 0.030315099285617732
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.26666666666666666,
                "acc_stderr,none": 0.026962424325073828
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.23841059602649006,
                "acc_stderr,none": 0.0347918557259966
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.35185185185185186,
                "acc_stderr,none": 0.03256850570293647
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.21428571428571427,
                "acc_stderr,none": 0.03894641120044792
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.25573280159521433,
                "acc_stderr,none": 0.003679818469152143,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.24973432518597238,
                "acc_stderr,none": 0.006309482198066187,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.2574831026713872,
                "acc_stderr,none": 0.007845140895775557,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.25706857328566785,
                "acc_stderr,none": 0.007866513295675203,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.26165556612749763,
                "acc_stderr,none": 0.007825131313123073,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=openai-community/gpt2-xl",
            "model_num_parameters": 1557611200,
            "model_dtype": "torch.float32",
            "model_revision": "main",
            "model_sha": "15ea56dee5df4983c59b2538573817e1667135e2",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721399086.3383083,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "50256"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "50256"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "50256"
        ],
        "eot_token_id": 50256,
        "max_length": 1024
    },
    "gpt2-xl_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.2963450450854722,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=openai-community/gpt2-xl",
            "model_num_parameters": 1557611200,
            "model_dtype": "torch.float32",
            "model_revision": "main",
            "model_sha": "15ea56dee5df4983c59b2538573817e1667135e2",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721389776.7835565,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "50256"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "50256"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "50256"
        ],
        "eot_token_id": 50256,
        "max_length": 1024
    },
    "gpt2_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.22924084888192564,
                "acc_stderr,none": 0.0035412774997792876,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.24229543039319873,
                "acc_stderr,none": 0.0062444989054601975,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2777777777777778,
                "acc_stderr,none": 0.04006168083848876
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.21818181818181817,
                "acc_stderr,none": 0.03225078108306289
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.25,
                "acc_stderr,none": 0.03039153369274154
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.270042194092827,
                "acc_stderr,none": 0.028900721906293426
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.2396694214876033,
                "acc_stderr,none": 0.03896878985070417
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.26851851851851855,
                "acc_stderr,none": 0.04284467968052192
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.22085889570552147,
                "acc_stderr,none": 0.032591773927421776
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.24566473988439305,
                "acc_stderr,none": 0.02317629820399201
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.23798882681564246,
                "acc_stderr,none": 0.014242630070574885
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.18971061093247588,
                "acc_stderr,none": 0.022268196258783225
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.21604938271604937,
                "acc_stderr,none": 0.022899162918445813
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.2457627118644068,
                "acc_stderr,none": 0.01099615663514269
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.32748538011695905,
                "acc_stderr,none": 0.035993357714560276
            },
            "mmlu_other": {
                "acc,none": 0.23817186997103315,
                "acc_stderr,none": 0.00762385975479662,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.20754716981132076,
                "acc_stderr,none": 0.02495991802891127
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.20809248554913296,
                "acc_stderr,none": 0.030952890217749884
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.18,
                "acc_stderr,none": 0.038612291966536955
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.31390134529147984,
                "acc_stderr,none": 0.031146796482972465
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.17475728155339806,
                "acc_stderr,none": 0.03760178006026621
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.2905982905982906,
                "acc_stderr,none": 0.029745048572674057
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.31,
                "acc_stderr,none": 0.04648231987117316
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.23371647509578544,
                "acc_stderr,none": 0.015133383278988846
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.21568627450980393,
                "acc_stderr,none": 0.023550831351995094
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.23049645390070922,
                "acc_stderr,none": 0.025123739226872405
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.1948529411764706,
                "acc_stderr,none": 0.024060599423487424
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.28313253012048195,
                "acc_stderr,none": 0.03507295431370518
            },
            "mmlu_social_sciences": {
                "acc,none": 0.216769580760481,
                "acc_stderr,none": 0.007424385141503122,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.23684210526315788,
                "acc_stderr,none": 0.039994238792813386
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.17676767676767677,
                "acc_stderr,none": 0.027178752639044915
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.19689119170984457,
                "acc_stderr,none": 0.02869787397186069
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.20256410256410257,
                "acc_stderr,none": 0.020377660970371397
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.21008403361344538,
                "acc_stderr,none": 0.026461398717471874
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.1908256880733945,
                "acc_stderr,none": 0.01684767640009109
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.2595419847328244,
                "acc_stderr,none": 0.03844876139785271
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.25,
                "acc_stderr,none": 0.01751781884501444
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.21818181818181817,
                "acc_stderr,none": 0.03955932861795833
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.18775510204081633,
                "acc_stderr,none": 0.02500025603954622
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.24378109452736318,
                "acc_stderr,none": 0.030360490154014652
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.28,
                "acc_stderr,none": 0.045126085985421276
            },
            "mmlu_stem": {
                "acc,none": 0.2131303520456708,
                "acc_stderr,none": 0.007277089042993837,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.22,
                "acc_stderr,none": 0.04163331998932269
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.2,
                "acc_stderr,none": 0.03455473702325437
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.17763157894736842,
                "acc_stderr,none": 0.031103182383123398
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2569444444444444,
                "acc_stderr,none": 0.03653946969442099
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.19,
                "acc_stderr,none": 0.03942772444036623
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.25,
                "acc_stderr,none": 0.04351941398892446
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.21568627450980393,
                "acc_stderr,none": 0.040925639582376556
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.28,
                "acc_stderr,none": 0.045126085985421276
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.26382978723404255,
                "acc_stderr,none": 0.02880998985410298
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2413793103448276,
                "acc_stderr,none": 0.03565998174135302
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.20899470899470898,
                "acc_stderr,none": 0.020940481565334835
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.1774193548387097,
                "acc_stderr,none": 0.021732540689329265
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.15270935960591134,
                "acc_stderr,none": 0.02530890453938062
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.25,
                "acc_stderr,none": 0.04351941398892446
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.2111111111111111,
                "acc_stderr,none": 0.02488211685765508
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.1986754966887417,
                "acc_stderr,none": 0.032578473844367746
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.1527777777777778,
                "acc_stderr,none": 0.02453632602613422
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.33035714285714285,
                "acc_stderr,none": 0.04464285714285713
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.22924084888192564,
                "acc_stderr,none": 0.0035412774997792876,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.24229543039319873,
                "acc_stderr,none": 0.0062444989054601975,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.23817186997103315,
                "acc_stderr,none": 0.00762385975479662,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.216769580760481,
                "acc_stderr,none": 0.007424385141503122,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.2131303520456708,
                "acc_stderr,none": 0.007277089042993837,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=openai-community/gpt2",
            "model_num_parameters": 124439808,
            "model_dtype": "torch.float32",
            "model_revision": "main",
            "model_sha": "607a30d783dfa663caf39e06633721c8d4cfcd7e",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721391168.4501317,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "50256"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "50256"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "50256"
        ],
        "eot_token_id": 50256,
        "max_length": 1024
    },
    "gpt2_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.29711878513128276,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=openai-community/gpt2",
            "model_num_parameters": 124439808,
            "model_dtype": "torch.float32",
            "model_revision": "main",
            "model_sha": "607a30d783dfa663caf39e06633721c8d4cfcd7e",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721389665.5962362,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "50256"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "50256"
        ],
        "tokenizer_bos_token": [
            "<|endoftext|>",
            "50256"
        ],
        "eot_token_id": 50256,
        "max_length": 1024
    },
    "Qwen1.5-0.5B_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.3662583677538812,
                "acc_stderr,none": 0.004013935898832145,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.3485653560042508,
                "acc_stderr,none": 0.006870928712115748,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.30952380952380953,
                "acc_stderr,none": 0.04134913018303316
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.48484848484848486,
                "acc_stderr,none": 0.03902551007374448
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.4215686274509804,
                "acc_stderr,none": 0.03465868196380758
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.5147679324894515,
                "acc_stderr,none": 0.032533028078777386
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.49586776859504134,
                "acc_stderr,none": 0.045641987674327526
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.37962962962962965,
                "acc_stderr,none": 0.04691521224077742
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.3496932515337423,
                "acc_stderr,none": 0.03746668325470022
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.43641618497109824,
                "acc_stderr,none": 0.026700545424943677
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.2536312849162011,
                "acc_stderr,none": 0.014551553659369922
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.36977491961414793,
                "acc_stderr,none": 0.027417996705630998
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.3611111111111111,
                "acc_stderr,none": 0.026725868809100786
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.3161668839634941,
                "acc_stderr,none": 0.01187578089438658
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.3508771929824561,
                "acc_stderr,none": 0.036602988340491645
            },
            "mmlu_other": {
                "acc,none": 0.41744448020598646,
                "acc_stderr,none": 0.008746434251578056,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.48,
                "acc_stderr,none": 0.050211673156867795
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.39622641509433965,
                "acc_stderr,none": 0.030102793781791197
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.34104046242774566,
                "acc_stderr,none": 0.03614665424180826
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.23,
                "acc_stderr,none": 0.04229525846816505
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.4080717488789238,
                "acc_stderr,none": 0.03298574607842821
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.46601941747572817,
                "acc_stderr,none": 0.0493929144727348
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.5982905982905983,
                "acc_stderr,none": 0.03211693751051622
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.46,
                "acc_stderr,none": 0.05009082659620333
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.4508301404853129,
                "acc_stderr,none": 0.01779329757269905
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.4477124183006536,
                "acc_stderr,none": 0.028472938478033526
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.35106382978723405,
                "acc_stderr,none": 0.028473501272963768
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.2867647058823529,
                "acc_stderr,none": 0.02747227447323382
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.42168674698795183,
                "acc_stderr,none": 0.03844453181770917
            },
            "mmlu_social_sciences": {
                "acc,none": 0.38511537211569713,
                "acc_stderr,none": 0.008670421510943325,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.2982456140350877,
                "acc_stderr,none": 0.04303684033537315
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.45454545454545453,
                "acc_stderr,none": 0.03547601494006936
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.47668393782383417,
                "acc_stderr,none": 0.03604513672442204
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.30256410256410254,
                "acc_stderr,none": 0.02329088805377272
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.31512605042016806,
                "acc_stderr,none": 0.030176808288974337
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.3963302752293578,
                "acc_stderr,none": 0.020971469947900525
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.4961832061068702,
                "acc_stderr,none": 0.043851623256015534
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.35130718954248363,
                "acc_stderr,none": 0.01931267606578656
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.35454545454545455,
                "acc_stderr,none": 0.045820048415054146
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.3183673469387755,
                "acc_stderr,none": 0.029822533793982052
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.4925373134328358,
                "acc_stderr,none": 0.03535140084276719
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.64,
                "acc_stderr,none": 0.04824181513244218
            },
            "mmlu_stem": {
                "acc,none": 0.3238185854741516,
                "acc_stderr,none": 0.008279563404873382,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.34,
                "acc_stderr,none": 0.04760952285695235
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.34814814814814815,
                "acc_stderr,none": 0.041153246103369526
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.35526315789473684,
                "acc_stderr,none": 0.03894734487013317
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.3611111111111111,
                "acc_stderr,none": 0.040166600304512336
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.23,
                "acc_stderr,none": 0.04229525846816508
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.31,
                "acc_stderr,none": 0.04648231987117316
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.29,
                "acc_stderr,none": 0.04560480215720684
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.28431372549019607,
                "acc_stderr,none": 0.04488482852329017
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.47,
                "acc_stderr,none": 0.050161355804659205
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.3276595744680851,
                "acc_stderr,none": 0.03068302084323101
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.4,
                "acc_stderr,none": 0.04082482904638627
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.3148148148148148,
                "acc_stderr,none": 0.023919984164047732
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.4161290322580645,
                "acc_stderr,none": 0.02804098138076154
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.2955665024630542,
                "acc_stderr,none": 0.032104944337514575
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.39,
                "acc_stderr,none": 0.049020713000019756
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.2814814814814815,
                "acc_stderr,none": 0.027420019350945273
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.2052980132450331,
                "acc_stderr,none": 0.03297986648473836
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.2175925925925926,
                "acc_stderr,none": 0.0281396894448597
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.3482142857142857,
                "acc_stderr,none": 0.04521829902833585
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.3662583677538812,
                "acc_stderr,none": 0.004013935898832145,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.3485653560042508,
                "acc_stderr,none": 0.006870928712115748,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.41744448020598646,
                "acc_stderr,none": 0.008746434251578056,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.38511537211569713,
                "acc_stderr,none": 0.008670421510943325,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.3238185854741516,
                "acc_stderr,none": 0.008279563404873382,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=Qwen/Qwen1.5-0.5B",
            "model_num_parameters": 463987712,
            "model_dtype": "torch.bfloat16",
            "model_revision": "main",
            "model_sha": "8f445e3628f3500ee69f24e1303c9f10f5342a39",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721413863.8811853,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "151643"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "151643"
        ],
        "tokenizer_bos_token": [
            null,
            "None"
        ],
        "eot_token_id": 151643,
        "max_length": 32768
    },
    "Qwen1.5-0.5B_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.47641199229916115,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=Qwen/Qwen1.5-0.5B",
            "model_num_parameters": 463987712,
            "model_dtype": "torch.bfloat16",
            "model_revision": "main",
            "model_sha": "8f445e3628f3500ee69f24e1303c9f10f5342a39",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721394704.131331,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "151643"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "151643"
        ],
        "tokenizer_bos_token": [
            null,
            "None"
        ],
        "eot_token_id": 151643,
        "max_length": 32768
    },
    "Qwen1.5-1.8B_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.4539239424583393,
                "acc_stderr,none": 0.004078614279628321,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.41466524973432517,
                "acc_stderr,none": 0.006983005072315248,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.3412698412698413,
                "acc_stderr,none": 0.042407993275749255
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.6,
                "acc_stderr,none": 0.038254602783800266
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.5343137254901961,
                "acc_stderr,none": 0.03501038327635897
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.6540084388185654,
                "acc_stderr,none": 0.030964810588786716
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.6198347107438017,
                "acc_stderr,none": 0.04431324501968431
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.5,
                "acc_stderr,none": 0.04833682445228318
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.39263803680981596,
                "acc_stderr,none": 0.03836740907831029
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.47398843930635837,
                "acc_stderr,none": 0.026882643434022885
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.2346368715083799,
                "acc_stderr,none": 0.014173044098303673
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.5016077170418006,
                "acc_stderr,none": 0.02839794490780661
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.4660493827160494,
                "acc_stderr,none": 0.027756535257347663
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.38070404172099087,
                "acc_stderr,none": 0.0124014306546459
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.5087719298245614,
                "acc_stderr,none": 0.038342347441649924
            },
            "mmlu_other": {
                "acc,none": 0.5300933376247183,
                "acc_stderr,none": 0.008743166729932776,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.56,
                "acc_stderr,none": 0.049888765156985884
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.49056603773584906,
                "acc_stderr,none": 0.0307673947078081
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.4624277456647399,
                "acc_stderr,none": 0.0380168510452446
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.25,
                "acc_stderr,none": 0.04351941398892446
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.547085201793722,
                "acc_stderr,none": 0.03340867501923324
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.6116504854368932,
                "acc_stderr,none": 0.04825729337356389
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.7264957264957265,
                "acc_stderr,none": 0.02920254015343118
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.58,
                "acc_stderr,none": 0.049604496374885836
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.5977011494252874,
                "acc_stderr,none": 0.01753529452906895
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.5947712418300654,
                "acc_stderr,none": 0.028110928492809075
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.31560283687943264,
                "acc_stderr,none": 0.02772498944950931
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.4852941176470588,
                "acc_stderr,none": 0.03035969707904612
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.43373493975903615,
                "acc_stderr,none": 0.03858158940685517
            },
            "mmlu_social_sciences": {
                "acc,none": 0.5086122846928827,
                "acc_stderr,none": 0.00885052252980608,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.2894736842105263,
                "acc_stderr,none": 0.04266339443159394
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.5808080808080808,
                "acc_stderr,none": 0.035155207286704175
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.6113989637305699,
                "acc_stderr,none": 0.03517739796373132
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.4153846153846154,
                "acc_stderr,none": 0.02498535492310235
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.42857142857142855,
                "acc_stderr,none": 0.032145368597886394
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.5889908256880734,
                "acc_stderr,none": 0.021095050687277652
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.5648854961832062,
                "acc_stderr,none": 0.04348208051644858
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.4199346405228758,
                "acc_stderr,none": 0.019966811178256483
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.4636363636363636,
                "acc_stderr,none": 0.047764491623961985
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.5387755102040817,
                "acc_stderr,none": 0.031912820526692774
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.6417910447761194,
                "acc_stderr,none": 0.03390393042268814
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.71,
                "acc_stderr,none": 0.045604802157206845
            },
            "mmlu_stem": {
                "acc,none": 0.3840786552489692,
                "acc_stderr,none": 0.008516347472033375,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.29,
                "acc_stderr,none": 0.045604802157206845
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.4,
                "acc_stderr,none": 0.042320736951515885
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.47368421052631576,
                "acc_stderr,none": 0.04063302731486671
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.4236111111111111,
                "acc_stderr,none": 0.041321250197233685
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.42,
                "acc_stderr,none": 0.049604496374885836
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.34,
                "acc_stderr,none": 0.04760952285695235
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.2647058823529412,
                "acc_stderr,none": 0.04389869956808781
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.58,
                "acc_stderr,none": 0.049604496374885836
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.3829787234042553,
                "acc_stderr,none": 0.03177821250236922
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.4413793103448276,
                "acc_stderr,none": 0.04137931034482758
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.3253968253968254,
                "acc_stderr,none": 0.024130158299762613
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.5580645161290323,
                "acc_stderr,none": 0.02825155790684974
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.39408866995073893,
                "acc_stderr,none": 0.034381579670365446
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.52,
                "acc_stderr,none": 0.05021167315686779
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.35555555555555557,
                "acc_stderr,none": 0.029185714949857406
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.25165562913907286,
                "acc_stderr,none": 0.035433042343899844
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.25,
                "acc_stderr,none": 0.029531221160930918
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.30357142857142855,
                "acc_stderr,none": 0.04364226155841044
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.4539239424583393,
                "acc_stderr,none": 0.004078614279628321,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.41466524973432517,
                "acc_stderr,none": 0.006983005072315248,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.5300933376247183,
                "acc_stderr,none": 0.008743166729932776,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.5086122846928827,
                "acc_stderr,none": 0.00885052252980608,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.3840786552489692,
                "acc_stderr,none": 0.008516347472033375,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=Qwen/Qwen1.5-1.8B",
            "model_num_parameters": 1836828672,
            "model_dtype": "torch.bfloat16",
            "model_revision": "main",
            "model_sha": "7846de7ed421727b318d6605a0bfab659da2c067",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721414596.3000293,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.001\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "151643"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "151643"
        ],
        "tokenizer_bos_token": [
            null,
            "None"
        ],
        "eot_token_id": 151643,
        "max_length": 32768
    },
    "Qwen1.5-1.8B_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.451130796250336,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=Qwen/Qwen1.5-1.8B",
            "model_num_parameters": 1836828672,
            "model_dtype": "torch.bfloat16",
            "model_revision": "main",
            "model_sha": "7846de7ed421727b318d6605a0bfab659da2c067",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721394780.7560205,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "151643"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "151643"
        ],
        "tokenizer_bos_token": [
            null,
            "None"
        ],
        "eot_token_id": 151643,
        "max_length": 32768
    },
    "Qwen1.5-14B_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.6570289132602194,
                "acc_stderr,none": 0.003774477239601439,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.5791710945802337,
                "acc_stderr,none": 0.006663947550539548,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.48412698412698413,
                "acc_stderr,none": 0.04469881854072606
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.806060606060606,
                "acc_stderr,none": 0.030874145136562076
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.7892156862745098,
                "acc_stderr,none": 0.028626547912437395
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.8396624472573839,
                "acc_stderr,none": 0.023884380925965672
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.8099173553719008,
                "acc_stderr,none": 0.035817969517092825
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.75,
                "acc_stderr,none": 0.04186091791394607
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.7852760736196319,
                "acc_stderr,none": 0.032262193772867744
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.7601156069364162,
                "acc_stderr,none": 0.02298959254312357
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.2916201117318436,
                "acc_stderr,none": 0.015201032512520422
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.6913183279742765,
                "acc_stderr,none": 0.026236965881153262
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.7160493827160493,
                "acc_stderr,none": 0.02508947852376513
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.49282920469361147,
                "acc_stderr,none": 0.012768922739553313
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.8011695906432749,
                "acc_stderr,none": 0.03061111655743253
            },
            "mmlu_other": {
                "acc,none": 0.7264242034116511,
                "acc_stderr,none": 0.007672582833164172,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.75,
                "acc_stderr,none": 0.04351941398892446
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.7358490566037735,
                "acc_stderr,none": 0.02713429162874171
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.6820809248554913,
                "acc_stderr,none": 0.0355068398916558
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.41,
                "acc_stderr,none": 0.049431107042371025
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.7040358744394619,
                "acc_stderr,none": 0.030636591348699813
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.8252427184466019,
                "acc_stderr,none": 0.0376017800602662
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.8974358974358975,
                "acc_stderr,none": 0.019875655027867464
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.77,
                "acc_stderr,none": 0.04229525846816507
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.8314176245210728,
                "acc_stderr,none": 0.013387895731543604
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.7581699346405228,
                "acc_stderr,none": 0.024518195641879334
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.5070921985815603,
                "acc_stderr,none": 0.02982449855912901
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.7132352941176471,
                "acc_stderr,none": 0.027472274473233818
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.4759036144578313,
                "acc_stderr,none": 0.03887971849597264
            },
            "mmlu_social_sciences": {
                "acc,none": 0.7539811504712383,
                "acc_stderr,none": 0.007645585959372935,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.5350877192982456,
                "acc_stderr,none": 0.046920083813689104
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.8383838383838383,
                "acc_stderr,none": 0.026225919863629293
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.8601036269430051,
                "acc_stderr,none": 0.02503387058301517
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.6897435897435897,
                "acc_stderr,none": 0.023454674889404288
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.7226890756302521,
                "acc_stderr,none": 0.02907937453948001
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.8293577981651377,
                "acc_stderr,none": 0.01612927102509986
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.7938931297709924,
                "acc_stderr,none": 0.03547771004159463
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.6879084967320261,
                "acc_stderr,none": 0.01874501120127766
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.6636363636363637,
                "acc_stderr,none": 0.04525393596302505
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.7673469387755102,
                "acc_stderr,none": 0.02704925791589618
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.8059701492537313,
                "acc_stderr,none": 0.0279626776047689
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.86,
                "acc_stderr,none": 0.03487350880197769
            },
            "mmlu_stem": {
                "acc,none": 0.6102124960355217,
                "acc_stderr,none": 0.00841770025447998,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.37,
                "acc_stderr,none": 0.048523658709391
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.6518518518518519,
                "acc_stderr,none": 0.041153246103369526
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.7171052631578947,
                "acc_stderr,none": 0.03665349695640767
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.7430555555555556,
                "acc_stderr,none": 0.03653946969442099
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.53,
                "acc_stderr,none": 0.050161355804659205
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.55,
                "acc_stderr,none": 0.05
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.44,
                "acc_stderr,none": 0.049888765156985884
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.4803921568627451,
                "acc_stderr,none": 0.04971358884367406
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.8,
                "acc_stderr,none": 0.04020151261036846
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.6893617021276596,
                "acc_stderr,none": 0.03025123757921317
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.6689655172413793,
                "acc_stderr,none": 0.039215453124671215
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.5846560846560847,
                "acc_stderr,none": 0.02537952491077839
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.8193548387096774,
                "acc_stderr,none": 0.021886178567172534
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.6206896551724138,
                "acc_stderr,none": 0.034139638059062345
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.75,
                "acc_stderr,none": 0.04351941398892446
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.44074074074074077,
                "acc_stderr,none": 0.03027067115728407
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.47019867549668876,
                "acc_stderr,none": 0.040752249922169775
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.5555555555555556,
                "acc_stderr,none": 0.03388857118502325
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.5089285714285714,
                "acc_stderr,none": 0.04745033255489123
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.6570289132602194,
                "acc_stderr,none": 0.003774477239601439,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.5791710945802337,
                "acc_stderr,none": 0.006663947550539548,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.7264242034116511,
                "acc_stderr,none": 0.007672582833164172,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.7539811504712383,
                "acc_stderr,none": 0.007645585959372935,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.6102124960355217,
                "acc_stderr,none": 0.00841770025447998,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=Qwen/Qwen1.5-14B",
            "model_num_parameters": 14167290880,
            "model_dtype": "torch.bfloat16",
            "model_revision": "main",
            "model_sha": "dce4b190d34470818e5bec2a92cb8233aaa02ca2",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721423348.300827,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "151643"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "151643"
        ],
        "tokenizer_bos_token": [
            null,
            "None"
        ],
        "eot_token_id": 151643,
        "max_length": 32768
    },
    "Qwen1.5-14B_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.6779600473635192,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=Qwen/Qwen1.5-14B",
            "model_num_parameters": 14167290880,
            "model_dtype": "torch.bfloat16",
            "model_revision": "main",
            "model_sha": "dce4b190d34470818e5bec2a92cb8233aaa02ca2",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721404019.5181363,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "151643"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "151643"
        ],
        "tokenizer_bos_token": [
            null,
            "None"
        ],
        "eot_token_id": 151643,
        "max_length": 32768
    },
    "Qwen1.5-4B_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.5420880216493377,
                "acc_stderr,none": 0.004029186940771914,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.4950053134962806,
                "acc_stderr,none": 0.006923795469984629,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.4523809523809524,
                "acc_stderr,none": 0.044518079590553275
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.7212121212121212,
                "acc_stderr,none": 0.03501438706296781
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.7156862745098039,
                "acc_stderr,none": 0.031660096793998116
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.729957805907173,
                "acc_stderr,none": 0.028900721906293426
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.7024793388429752,
                "acc_stderr,none": 0.04173349148083499
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.6944444444444444,
                "acc_stderr,none": 0.044531975073749834
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.6257668711656442,
                "acc_stderr,none": 0.03802068102899615
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.6213872832369942,
                "acc_stderr,none": 0.02611374936131034
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.2659217877094972,
                "acc_stderr,none": 0.01477676506643889
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.5594855305466238,
                "acc_stderr,none": 0.028196400574197422
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.6049382716049383,
                "acc_stderr,none": 0.027201117666925654
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.41264667535853977,
                "acc_stderr,none": 0.012573836633799011
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.6783625730994152,
                "acc_stderr,none": 0.03582529442573122
            },
            "mmlu_other": {
                "acc,none": 0.6063727067911169,
                "acc_stderr,none": 0.008493848394784542,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.66,
                "acc_stderr,none": 0.04760952285695237
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.5811320754716981,
                "acc_stderr,none": 0.030365050829115205
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.5317919075144508,
                "acc_stderr,none": 0.03804749744364764
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.37,
                "acc_stderr,none": 0.04852365870939099
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.6188340807174888,
                "acc_stderr,none": 0.03259625118416827
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.7087378640776699,
                "acc_stderr,none": 0.04498676320572924
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.8205128205128205,
                "acc_stderr,none": 0.02514093595033544
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.6,
                "acc_stderr,none": 0.04923659639173309
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.722860791826309,
                "acc_stderr,none": 0.016005636294122414
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.6013071895424836,
                "acc_stderr,none": 0.02803609227389176
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.4219858156028369,
                "acc_stderr,none": 0.0294621892333706
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.47058823529411764,
                "acc_stderr,none": 0.03032024326500413
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.45180722891566266,
                "acc_stderr,none": 0.03874371556587953
            },
            "mmlu_social_sciences": {
                "acc,none": 0.6265843353916152,
                "acc_stderr,none": 0.00854192901942222,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.32456140350877194,
                "acc_stderr,none": 0.04404556157374768
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.7323232323232324,
                "acc_stderr,none": 0.03154449888270286
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.7357512953367875,
                "acc_stderr,none": 0.03182155050916647
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.5615384615384615,
                "acc_stderr,none": 0.02515826601686857
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.5336134453781513,
                "acc_stderr,none": 0.03240501447690071
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.7027522935779816,
                "acc_stderr,none": 0.01959570722464353
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.6946564885496184,
                "acc_stderr,none": 0.04039314978724562
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.545751633986928,
                "acc_stderr,none": 0.0201429745537952
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.6272727272727273,
                "acc_stderr,none": 0.04631381319425465
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.6244897959183674,
                "acc_stderr,none": 0.031001209039894836
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.736318407960199,
                "acc_stderr,none": 0.031157150869355558
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.8,
                "acc_stderr,none": 0.04020151261036846
            },
            "mmlu_stem": {
                "acc,none": 0.46653980336187756,
                "acc_stderr,none": 0.008700968928117028,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.37,
                "acc_stderr,none": 0.048523658709391
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.4666666666666667,
                "acc_stderr,none": 0.043097329010363554
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.5855263157894737,
                "acc_stderr,none": 0.04008973785779206
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.5277777777777778,
                "acc_stderr,none": 0.04174752578923185
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.37,
                "acc_stderr,none": 0.04852365870939099
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.4,
                "acc_stderr,none": 0.04923659639173309
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.4,
                "acc_stderr,none": 0.04923659639173309
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.29411764705882354,
                "acc_stderr,none": 0.04533838195929774
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.68,
                "acc_stderr,none": 0.04688261722621504
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.48936170212765956,
                "acc_stderr,none": 0.03267862331014063
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.5103448275862069,
                "acc_stderr,none": 0.04165774775728763
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.4417989417989418,
                "acc_stderr,none": 0.025576257061253833
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.6806451612903226,
                "acc_stderr,none": 0.026522709674667775
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.45320197044334976,
                "acc_stderr,none": 0.035025446508458714
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.57,
                "acc_stderr,none": 0.049756985195624284
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.34814814814814815,
                "acc_stderr,none": 0.029045600290616258
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.3576158940397351,
                "acc_stderr,none": 0.03913453431177258
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.3888888888888889,
                "acc_stderr,none": 0.03324708911809117
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.38392857142857145,
                "acc_stderr,none": 0.04616143075028546
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.5420880216493377,
                "acc_stderr,none": 0.004029186940771914,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.4950053134962806,
                "acc_stderr,none": 0.006923795469984629,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.6063727067911169,
                "acc_stderr,none": 0.008493848394784542,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.6265843353916152,
                "acc_stderr,none": 0.00854192901942222,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.46653980336187756,
                "acc_stderr,none": 0.008700968928117028,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=Qwen/Qwen1.5-4B",
            "model_num_parameters": 3950369280,
            "model_dtype": "torch.bfloat16",
            "model_revision": "main",
            "model_sha": "a66363a0c24e2155c561e4b53c658b1d3965474e",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721415387.5393767,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.001\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "151643"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "151643"
        ],
        "tokenizer_bos_token": [
            null,
            "None"
        ],
        "eot_token_id": 151643,
        "max_length": 32768
    },
    "Qwen1.5-4B_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.5758425871504246,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=Qwen/Qwen1.5-4B",
            "model_num_parameters": 3950369280,
            "model_dtype": "torch.bfloat16",
            "model_revision": "main",
            "model_sha": "a66363a0c24e2155c561e4b53c658b1d3965474e",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721395256.2564156,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "151643"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "151643"
        ],
        "tokenizer_bos_token": [
            null,
            "None"
        ],
        "eot_token_id": 151643,
        "max_length": 32768
    },
    "Qwen1.5-7B_mmlu": {
        "results": {
            "mmlu": {
                "acc,none": 0.5979205241418601,
                "acc_stderr,none": 0.003937891958692374,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.5411264612114771,
                "acc_stderr,none": 0.006858711093369533,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.4523809523809524,
                "acc_stderr,none": 0.044518079590553275
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.7090909090909091,
                "acc_stderr,none": 0.03546563019624337
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.7843137254901961,
                "acc_stderr,none": 0.028867431449849303
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.7932489451476793,
                "acc_stderr,none": 0.0263616516683891
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.8016528925619835,
                "acc_stderr,none": 0.036401182719909476
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.7685185185185185,
                "acc_stderr,none": 0.04077494709252628
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.6748466257668712,
                "acc_stderr,none": 0.03680350371286464
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.653179190751445,
                "acc_stderr,none": 0.025624723994030457
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.3229050279329609,
                "acc_stderr,none": 0.015638440380241488
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.6591639871382636,
                "acc_stderr,none": 0.026920841260776155
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.6697530864197531,
                "acc_stderr,none": 0.02616829845673284
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.4367666232073012,
                "acc_stderr,none": 0.012667701919603662
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.7426900584795322,
                "acc_stderr,none": 0.03352799844161865
            },
            "mmlu_other": {
                "acc,none": 0.6517541036369489,
                "acc_stderr,none": 0.008244023825214225,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.65,
                "acc_stderr,none": 0.047937248544110196
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.660377358490566,
                "acc_stderr,none": 0.029146904747798325
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.6069364161849711,
                "acc_stderr,none": 0.0372424959581773
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.36,
                "acc_stderr,none": 0.04824181513244218
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.6188340807174888,
                "acc_stderr,none": 0.03259625118416827
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.7572815533980582,
                "acc_stderr,none": 0.04245022486384495
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.8675213675213675,
                "acc_stderr,none": 0.02220930907316562
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.68,
                "acc_stderr,none": 0.04688261722621504
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.7650063856960408,
                "acc_stderr,none": 0.01516202415227844
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.6503267973856209,
                "acc_stderr,none": 0.027305308076274695
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.42907801418439717,
                "acc_stderr,none": 0.02952591430255856
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.5882352941176471,
                "acc_stderr,none": 0.029896163033125468
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.46987951807228917,
                "acc_stderr,none": 0.03885425420866767
            },
            "mmlu_social_sciences": {
                "acc,none": 0.6945076373090673,
                "acc_stderr,none": 0.00808575171299895,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.45614035087719296,
                "acc_stderr,none": 0.046854730419077895
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.7828282828282829,
                "acc_stderr,none": 0.029376616484945633
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.8186528497409327,
                "acc_stderr,none": 0.027807032360686088
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.5871794871794872,
                "acc_stderr,none": 0.02496268356433179
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.6596638655462185,
                "acc_stderr,none": 0.03077805742293167
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.8,
                "acc_stderr,none": 0.017149858514250934
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.7480916030534351,
                "acc_stderr,none": 0.038073871163060866
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.5833333333333334,
                "acc_stderr,none": 0.01994491413687358
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.6181818181818182,
                "acc_stderr,none": 0.046534298079135075
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.7224489795918367,
                "acc_stderr,none": 0.02866685779027465
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.8208955223880597,
                "acc_stderr,none": 0.027113286753111837
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.85,
                "acc_stderr,none": 0.0358870281282637
            },
            "mmlu_stem": {
                "acc,none": 0.5353631462099587,
                "acc_stderr,none": 0.008635003579564956,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.38,
                "acc_stderr,none": 0.04878317312145633
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.5111111111111111,
                "acc_stderr,none": 0.04318275491977976
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.6842105263157895,
                "acc_stderr,none": 0.037827289808654685
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.6875,
                "acc_stderr,none": 0.038760854559127644
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.44,
                "acc_stderr,none": 0.04988876515698589
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.57,
                "acc_stderr,none": 0.04975698519562428
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.39,
                "acc_stderr,none": 0.04902071300001975
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.4215686274509804,
                "acc_stderr,none": 0.049135952012744975
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.73,
                "acc_stderr,none": 0.0446196043338474
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.5531914893617021,
                "acc_stderr,none": 0.0325005368436584
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.5586206896551724,
                "acc_stderr,none": 0.04137931034482757
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.4894179894179894,
                "acc_stderr,none": 0.02574554227604548
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.7225806451612903,
                "acc_stderr,none": 0.02547019683590005
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.5714285714285714,
                "acc_stderr,none": 0.034819048444388045
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.72,
                "acc_stderr,none": 0.045126085985421276
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.35555555555555557,
                "acc_stderr,none": 0.02918571494985741
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.3576158940397351,
                "acc_stderr,none": 0.03913453431177258
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.5324074074074074,
                "acc_stderr,none": 0.03402801581358966
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.4375,
                "acc_stderr,none": 0.04708567521880525
            }
        },
        "groups": {
            "mmlu": {
                "acc,none": 0.5979205241418601,
                "acc_stderr,none": 0.003937891958692374,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.5411264612114771,
                "acc_stderr,none": 0.006858711093369533,
                "alias": " - humanities"
            },
            "mmlu_other": {
                "acc,none": 0.6517541036369489,
                "acc_stderr,none": 0.008244023825214225,
                "alias": " - other"
            },
            "mmlu_social_sciences": {
                "acc,none": 0.6945076373090673,
                "acc_stderr,none": 0.00808575171299895,
                "alias": " - social sciences"
            },
            "mmlu_stem": {
                "acc,none": 0.5353631462099587,
                "acc_stderr,none": 0.008635003579564956,
                "alias": " - stem"
            }
        },
        "group_subtasks": {
            "mmlu_humanities": [
                "mmlu_international_law",
                "mmlu_professional_law",
                "mmlu_high_school_world_history",
                "mmlu_jurisprudence",
                "mmlu_moral_scenarios",
                "mmlu_high_school_european_history",
                "mmlu_logical_fallacies",
                "mmlu_prehistory",
                "mmlu_world_religions",
                "mmlu_moral_disputes",
                "mmlu_philosophy",
                "mmlu_high_school_us_history",
                "mmlu_formal_logic"
            ],
            "mmlu_social_sciences": [
                "mmlu_high_school_microeconomics",
                "mmlu_professional_psychology",
                "mmlu_security_studies",
                "mmlu_high_school_geography",
                "mmlu_high_school_government_and_politics",
                "mmlu_high_school_psychology",
                "mmlu_high_school_macroeconomics",
                "mmlu_econometrics",
                "mmlu_us_foreign_policy",
                "mmlu_public_relations",
                "mmlu_human_sexuality",
                "mmlu_sociology"
            ],
            "mmlu_other": [
                "mmlu_virology",
                "mmlu_miscellaneous",
                "mmlu_professional_medicine",
                "mmlu_marketing",
                "mmlu_clinical_knowledge",
                "mmlu_medical_genetics",
                "mmlu_human_aging",
                "mmlu_management",
                "mmlu_global_facts",
                "mmlu_professional_accounting",
                "mmlu_business_ethics",
                "mmlu_college_medicine",
                "mmlu_nutrition"
            ],
            "mmlu_stem": [
                "mmlu_high_school_statistics",
                "mmlu_abstract_algebra",
                "mmlu_high_school_biology",
                "mmlu_college_chemistry",
                "mmlu_high_school_physics",
                "mmlu_high_school_computer_science",
                "mmlu_college_mathematics",
                "mmlu_conceptual_physics",
                "mmlu_college_biology",
                "mmlu_electrical_engineering",
                "mmlu_high_school_chemistry",
                "mmlu_astronomy",
                "mmlu_elementary_mathematics",
                "mmlu_college_computer_science",
                "mmlu_computer_security",
                "mmlu_college_physics",
                "mmlu_high_school_mathematics",
                "mmlu_anatomy",
                "mmlu_machine_learning"
            ],
            "mmlu": [
                "mmlu_stem",
                "mmlu_other",
                "mmlu_social_sciences",
                "mmlu_humanities"
            ]
        },
        "configs": {
            "mmlu_abstract_algebra": {
                "task": "mmlu_abstract_algebra",
                "task_alias": "abstract_algebra",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "abstract_algebra",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_anatomy": {
                "task": "mmlu_anatomy",
                "task_alias": "anatomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "anatomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_astronomy": {
                "task": "mmlu_astronomy",
                "task_alias": "astronomy",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "astronomy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_business_ethics": {
                "task": "mmlu_business_ethics",
                "task_alias": "business_ethics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "business_ethics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_clinical_knowledge": {
                "task": "mmlu_clinical_knowledge",
                "task_alias": "clinical_knowledge",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "clinical_knowledge",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_biology": {
                "task": "mmlu_college_biology",
                "task_alias": "college_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_chemistry": {
                "task": "mmlu_college_chemistry",
                "task_alias": "college_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_computer_science": {
                "task": "mmlu_college_computer_science",
                "task_alias": "college_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_mathematics": {
                "task": "mmlu_college_mathematics",
                "task_alias": "college_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_medicine": {
                "task": "mmlu_college_medicine",
                "task_alias": "college_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_college_physics": {
                "task": "mmlu_college_physics",
                "task_alias": "college_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "college_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_computer_security": {
                "task": "mmlu_computer_security",
                "task_alias": "computer_security",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "computer_security",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_conceptual_physics": {
                "task": "mmlu_conceptual_physics",
                "task_alias": "conceptual_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "conceptual_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_econometrics": {
                "task": "mmlu_econometrics",
                "task_alias": "econometrics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "econometrics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_electrical_engineering": {
                "task": "mmlu_electrical_engineering",
                "task_alias": "electrical_engineering",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "electrical_engineering",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_elementary_mathematics": {
                "task": "mmlu_elementary_mathematics",
                "task_alias": "elementary_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "elementary_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_formal_logic": {
                "task": "mmlu_formal_logic",
                "task_alias": "formal_logic",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "formal_logic",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_global_facts": {
                "task": "mmlu_global_facts",
                "task_alias": "global_facts",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "global_facts",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_biology": {
                "task": "mmlu_high_school_biology",
                "task_alias": "high_school_biology",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_biology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_chemistry": {
                "task": "mmlu_high_school_chemistry",
                "task_alias": "high_school_chemistry",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_chemistry",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_computer_science": {
                "task": "mmlu_high_school_computer_science",
                "task_alias": "high_school_computer_science",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_computer_science",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_european_history": {
                "task": "mmlu_high_school_european_history",
                "task_alias": "high_school_european_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_european_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_geography": {
                "task": "mmlu_high_school_geography",
                "task_alias": "high_school_geography",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_geography",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_government_and_politics": {
                "task": "mmlu_high_school_government_and_politics",
                "task_alias": "high_school_government_and_politics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_government_and_politics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_macroeconomics": {
                "task": "mmlu_high_school_macroeconomics",
                "task_alias": "high_school_macroeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_macroeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_mathematics": {
                "task": "mmlu_high_school_mathematics",
                "task_alias": "high_school_mathematics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_mathematics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_microeconomics": {
                "task": "mmlu_high_school_microeconomics",
                "task_alias": "high_school_microeconomics",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_microeconomics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_physics": {
                "task": "mmlu_high_school_physics",
                "task_alias": "high_school_physics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_physics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_psychology": {
                "task": "mmlu_high_school_psychology",
                "task_alias": "high_school_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_statistics": {
                "task": "mmlu_high_school_statistics",
                "task_alias": "high_school_statistics",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_statistics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_us_history": {
                "task": "mmlu_high_school_us_history",
                "task_alias": "high_school_us_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_us_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_high_school_world_history": {
                "task": "mmlu_high_school_world_history",
                "task_alias": "high_school_world_history",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "high_school_world_history",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_aging": {
                "task": "mmlu_human_aging",
                "task_alias": "human_aging",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_aging",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_human_sexuality": {
                "task": "mmlu_human_sexuality",
                "task_alias": "human_sexuality",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "human_sexuality",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_international_law": {
                "task": "mmlu_international_law",
                "task_alias": "international_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "international_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about international law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_jurisprudence": {
                "task": "mmlu_jurisprudence",
                "task_alias": "jurisprudence",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "jurisprudence",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_logical_fallacies": {
                "task": "mmlu_logical_fallacies",
                "task_alias": "logical_fallacies",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "logical_fallacies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_machine_learning": {
                "task": "mmlu_machine_learning",
                "task_alias": "machine_learning",
                "tag": "mmlu_stem_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "machine_learning",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_management": {
                "task": "mmlu_management",
                "task_alias": "management",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "management",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about management.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_marketing": {
                "task": "mmlu_marketing",
                "task_alias": "marketing",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "marketing",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_medical_genetics": {
                "task": "mmlu_medical_genetics",
                "task_alias": "medical_genetics",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "medical_genetics",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_miscellaneous": {
                "task": "mmlu_miscellaneous",
                "task_alias": "miscellaneous",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "miscellaneous",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_disputes": {
                "task": "mmlu_moral_disputes",
                "task_alias": "moral_disputes",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_disputes",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_moral_scenarios": {
                "task": "mmlu_moral_scenarios",
                "task_alias": "moral_scenarios",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "moral_scenarios",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_nutrition": {
                "task": "mmlu_nutrition",
                "task_alias": "nutrition",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "nutrition",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_philosophy": {
                "task": "mmlu_philosophy",
                "task_alias": "philosophy",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "philosophy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_prehistory": {
                "task": "mmlu_prehistory",
                "task_alias": "prehistory",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "prehistory",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_accounting": {
                "task": "mmlu_professional_accounting",
                "task_alias": "professional_accounting",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_accounting",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_law": {
                "task": "mmlu_professional_law",
                "task_alias": "professional_law",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_law",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_medicine": {
                "task": "mmlu_professional_medicine",
                "task_alias": "professional_medicine",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_medicine",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_professional_psychology": {
                "task": "mmlu_professional_psychology",
                "task_alias": "professional_psychology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "professional_psychology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_public_relations": {
                "task": "mmlu_public_relations",
                "task_alias": "public_relations",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "public_relations",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_security_studies": {
                "task": "mmlu_security_studies",
                "task_alias": "security_studies",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "security_studies",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_sociology": {
                "task": "mmlu_sociology",
                "task_alias": "sociology",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "sociology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_us_foreign_policy": {
                "task": "mmlu_us_foreign_policy",
                "task_alias": "us_foreign_policy",
                "tag": "mmlu_social_sciences_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "us_foreign_policy",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_virology": {
                "task": "mmlu_virology",
                "task_alias": "virology",
                "tag": "mmlu_other_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "virology",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about virology.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            },
            "mmlu_world_religions": {
                "task": "mmlu_world_religions",
                "task_alias": "world_religions",
                "tag": "mmlu_humanities_tasks",
                "dataset_path": "hails/mmlu_no_train",
                "dataset_name": "world_religions",
                "dataset_kwargs": {
                    "trust_remote_code": true
                },
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc",
                        "aggregation": "mean",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "mmlu": 1,
            "mmlu_abstract_algebra": 0.0,
            "mmlu_anatomy": 0.0,
            "mmlu_astronomy": 0.0,
            "mmlu_business_ethics": 0.0,
            "mmlu_clinical_knowledge": 0.0,
            "mmlu_college_biology": 0.0,
            "mmlu_college_chemistry": 0.0,
            "mmlu_college_computer_science": 0.0,
            "mmlu_college_mathematics": 0.0,
            "mmlu_college_medicine": 0.0,
            "mmlu_college_physics": 0.0,
            "mmlu_computer_security": 0.0,
            "mmlu_conceptual_physics": 0.0,
            "mmlu_econometrics": 0.0,
            "mmlu_electrical_engineering": 0.0,
            "mmlu_elementary_mathematics": 0.0,
            "mmlu_formal_logic": 0.0,
            "mmlu_global_facts": 0.0,
            "mmlu_high_school_biology": 0.0,
            "mmlu_high_school_chemistry": 0.0,
            "mmlu_high_school_computer_science": 0.0,
            "mmlu_high_school_european_history": 0.0,
            "mmlu_high_school_geography": 0.0,
            "mmlu_high_school_government_and_politics": 0.0,
            "mmlu_high_school_macroeconomics": 0.0,
            "mmlu_high_school_mathematics": 0.0,
            "mmlu_high_school_microeconomics": 0.0,
            "mmlu_high_school_physics": 0.0,
            "mmlu_high_school_psychology": 0.0,
            "mmlu_high_school_statistics": 0.0,
            "mmlu_high_school_us_history": 0.0,
            "mmlu_high_school_world_history": 0.0,
            "mmlu_human_aging": 0.0,
            "mmlu_human_sexuality": 0.0,
            "mmlu_humanities": 1,
            "mmlu_international_law": 0.0,
            "mmlu_jurisprudence": 0.0,
            "mmlu_logical_fallacies": 0.0,
            "mmlu_machine_learning": 0.0,
            "mmlu_management": 0.0,
            "mmlu_marketing": 0.0,
            "mmlu_medical_genetics": 0.0,
            "mmlu_miscellaneous": 0.0,
            "mmlu_moral_disputes": 0.0,
            "mmlu_moral_scenarios": 0.0,
            "mmlu_nutrition": 0.0,
            "mmlu_other": 1,
            "mmlu_philosophy": 0.0,
            "mmlu_prehistory": 0.0,
            "mmlu_professional_accounting": 0.0,
            "mmlu_professional_law": 0.0,
            "mmlu_professional_medicine": 0.0,
            "mmlu_professional_psychology": 0.0,
            "mmlu_public_relations": 0.0,
            "mmlu_security_studies": 0.0,
            "mmlu_social_sciences": 1,
            "mmlu_sociology": 0.0,
            "mmlu_stem": 1,
            "mmlu_us_foreign_policy": 0.0,
            "mmlu_virology": 0.0,
            "mmlu_world_religions": 0.0
        },
        "n-shot": {
            "mmlu_abstract_algebra": 0,
            "mmlu_anatomy": 0,
            "mmlu_astronomy": 0,
            "mmlu_business_ethics": 0,
            "mmlu_clinical_knowledge": 0,
            "mmlu_college_biology": 0,
            "mmlu_college_chemistry": 0,
            "mmlu_college_computer_science": 0,
            "mmlu_college_mathematics": 0,
            "mmlu_college_medicine": 0,
            "mmlu_college_physics": 0,
            "mmlu_computer_security": 0,
            "mmlu_conceptual_physics": 0,
            "mmlu_econometrics": 0,
            "mmlu_electrical_engineering": 0,
            "mmlu_elementary_mathematics": 0,
            "mmlu_formal_logic": 0,
            "mmlu_global_facts": 0,
            "mmlu_high_school_biology": 0,
            "mmlu_high_school_chemistry": 0,
            "mmlu_high_school_computer_science": 0,
            "mmlu_high_school_european_history": 0,
            "mmlu_high_school_geography": 0,
            "mmlu_high_school_government_and_politics": 0,
            "mmlu_high_school_macroeconomics": 0,
            "mmlu_high_school_mathematics": 0,
            "mmlu_high_school_microeconomics": 0,
            "mmlu_high_school_physics": 0,
            "mmlu_high_school_psychology": 0,
            "mmlu_high_school_statistics": 0,
            "mmlu_high_school_us_history": 0,
            "mmlu_high_school_world_history": 0,
            "mmlu_human_aging": 0,
            "mmlu_human_sexuality": 0,
            "mmlu_international_law": 0,
            "mmlu_jurisprudence": 0,
            "mmlu_logical_fallacies": 0,
            "mmlu_machine_learning": 0,
            "mmlu_management": 0,
            "mmlu_marketing": 0,
            "mmlu_medical_genetics": 0,
            "mmlu_miscellaneous": 0,
            "mmlu_moral_disputes": 0,
            "mmlu_moral_scenarios": 0,
            "mmlu_nutrition": 0,
            "mmlu_philosophy": 0,
            "mmlu_prehistory": 0,
            "mmlu_professional_accounting": 0,
            "mmlu_professional_law": 0,
            "mmlu_professional_medicine": 0,
            "mmlu_professional_psychology": 0,
            "mmlu_public_relations": 0,
            "mmlu_security_studies": 0,
            "mmlu_sociology": 0,
            "mmlu_us_foreign_policy": 0,
            "mmlu_virology": 0,
            "mmlu_world_religions": 0
        },
        "higher_is_better": {
            "mmlu": {
                "acc": true
            },
            "mmlu_abstract_algebra": {
                "acc": true
            },
            "mmlu_anatomy": {
                "acc": true
            },
            "mmlu_astronomy": {
                "acc": true
            },
            "mmlu_business_ethics": {
                "acc": true
            },
            "mmlu_clinical_knowledge": {
                "acc": true
            },
            "mmlu_college_biology": {
                "acc": true
            },
            "mmlu_college_chemistry": {
                "acc": true
            },
            "mmlu_college_computer_science": {
                "acc": true
            },
            "mmlu_college_mathematics": {
                "acc": true
            },
            "mmlu_college_medicine": {
                "acc": true
            },
            "mmlu_college_physics": {
                "acc": true
            },
            "mmlu_computer_security": {
                "acc": true
            },
            "mmlu_conceptual_physics": {
                "acc": true
            },
            "mmlu_econometrics": {
                "acc": true
            },
            "mmlu_electrical_engineering": {
                "acc": true
            },
            "mmlu_elementary_mathematics": {
                "acc": true
            },
            "mmlu_formal_logic": {
                "acc": true
            },
            "mmlu_global_facts": {
                "acc": true
            },
            "mmlu_high_school_biology": {
                "acc": true
            },
            "mmlu_high_school_chemistry": {
                "acc": true
            },
            "mmlu_high_school_computer_science": {
                "acc": true
            },
            "mmlu_high_school_european_history": {
                "acc": true
            },
            "mmlu_high_school_geography": {
                "acc": true
            },
            "mmlu_high_school_government_and_politics": {
                "acc": true
            },
            "mmlu_high_school_macroeconomics": {
                "acc": true
            },
            "mmlu_high_school_mathematics": {
                "acc": true
            },
            "mmlu_high_school_microeconomics": {
                "acc": true
            },
            "mmlu_high_school_physics": {
                "acc": true
            },
            "mmlu_high_school_psychology": {
                "acc": true
            },
            "mmlu_high_school_statistics": {
                "acc": true
            },
            "mmlu_high_school_us_history": {
                "acc": true
            },
            "mmlu_high_school_world_history": {
                "acc": true
            },
            "mmlu_human_aging": {
                "acc": true
            },
            "mmlu_human_sexuality": {
                "acc": true
            },
            "mmlu_humanities": {
                "acc": true
            },
            "mmlu_international_law": {
                "acc": true
            },
            "mmlu_jurisprudence": {
                "acc": true
            },
            "mmlu_logical_fallacies": {
                "acc": true
            },
            "mmlu_machine_learning": {
                "acc": true
            },
            "mmlu_management": {
                "acc": true
            },
            "mmlu_marketing": {
                "acc": true
            },
            "mmlu_medical_genetics": {
                "acc": true
            },
            "mmlu_miscellaneous": {
                "acc": true
            },
            "mmlu_moral_disputes": {
                "acc": true
            },
            "mmlu_moral_scenarios": {
                "acc": true
            },
            "mmlu_nutrition": {
                "acc": true
            },
            "mmlu_other": {
                "acc": true
            },
            "mmlu_philosophy": {
                "acc": true
            },
            "mmlu_prehistory": {
                "acc": true
            },
            "mmlu_professional_accounting": {
                "acc": true
            },
            "mmlu_professional_law": {
                "acc": true
            },
            "mmlu_professional_medicine": {
                "acc": true
            },
            "mmlu_professional_psychology": {
                "acc": true
            },
            "mmlu_public_relations": {
                "acc": true
            },
            "mmlu_security_studies": {
                "acc": true
            },
            "mmlu_social_sciences": {
                "acc": true
            },
            "mmlu_sociology": {
                "acc": true
            },
            "mmlu_stem": {
                "acc": true
            },
            "mmlu_us_foreign_policy": {
                "acc": true
            },
            "mmlu_virology": {
                "acc": true
            },
            "mmlu_world_religions": {
                "acc": true
            }
        },
        "n-samples": {
            "mmlu_high_school_statistics": {
                "original": 216,
                "effective": 216
            },
            "mmlu_abstract_algebra": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_biology": {
                "original": 310,
                "effective": 310
            },
            "mmlu_college_chemistry": {
                "original": 100,
                "effective": 100
            },
            "mmlu_high_school_physics": {
                "original": 151,
                "effective": 151
            },
            "mmlu_high_school_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_mathematics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_conceptual_physics": {
                "original": 235,
                "effective": 235
            },
            "mmlu_college_biology": {
                "original": 144,
                "effective": 144
            },
            "mmlu_electrical_engineering": {
                "original": 145,
                "effective": 145
            },
            "mmlu_high_school_chemistry": {
                "original": 203,
                "effective": 203
            },
            "mmlu_astronomy": {
                "original": 152,
                "effective": 152
            },
            "mmlu_elementary_mathematics": {
                "original": 378,
                "effective": 378
            },
            "mmlu_college_computer_science": {
                "original": 100,
                "effective": 100
            },
            "mmlu_computer_security": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_physics": {
                "original": 102,
                "effective": 102
            },
            "mmlu_high_school_mathematics": {
                "original": 270,
                "effective": 270
            },
            "mmlu_anatomy": {
                "original": 135,
                "effective": 135
            },
            "mmlu_machine_learning": {
                "original": 112,
                "effective": 112
            },
            "mmlu_virology": {
                "original": 166,
                "effective": 166
            },
            "mmlu_miscellaneous": {
                "original": 783,
                "effective": 783
            },
            "mmlu_professional_medicine": {
                "original": 272,
                "effective": 272
            },
            "mmlu_marketing": {
                "original": 234,
                "effective": 234
            },
            "mmlu_clinical_knowledge": {
                "original": 265,
                "effective": 265
            },
            "mmlu_medical_genetics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_human_aging": {
                "original": 223,
                "effective": 223
            },
            "mmlu_management": {
                "original": 103,
                "effective": 103
            },
            "mmlu_global_facts": {
                "original": 100,
                "effective": 100
            },
            "mmlu_professional_accounting": {
                "original": 282,
                "effective": 282
            },
            "mmlu_business_ethics": {
                "original": 100,
                "effective": 100
            },
            "mmlu_college_medicine": {
                "original": 173,
                "effective": 173
            },
            "mmlu_nutrition": {
                "original": 306,
                "effective": 306
            },
            "mmlu_high_school_microeconomics": {
                "original": 238,
                "effective": 238
            },
            "mmlu_professional_psychology": {
                "original": 612,
                "effective": 612
            },
            "mmlu_security_studies": {
                "original": 245,
                "effective": 245
            },
            "mmlu_high_school_geography": {
                "original": 198,
                "effective": 198
            },
            "mmlu_high_school_government_and_politics": {
                "original": 193,
                "effective": 193
            },
            "mmlu_high_school_psychology": {
                "original": 545,
                "effective": 545
            },
            "mmlu_high_school_macroeconomics": {
                "original": 390,
                "effective": 390
            },
            "mmlu_econometrics": {
                "original": 114,
                "effective": 114
            },
            "mmlu_us_foreign_policy": {
                "original": 100,
                "effective": 100
            },
            "mmlu_public_relations": {
                "original": 110,
                "effective": 110
            },
            "mmlu_human_sexuality": {
                "original": 131,
                "effective": 131
            },
            "mmlu_sociology": {
                "original": 201,
                "effective": 201
            },
            "mmlu_international_law": {
                "original": 121,
                "effective": 121
            },
            "mmlu_professional_law": {
                "original": 1534,
                "effective": 1534
            },
            "mmlu_high_school_world_history": {
                "original": 237,
                "effective": 237
            },
            "mmlu_jurisprudence": {
                "original": 108,
                "effective": 108
            },
            "mmlu_moral_scenarios": {
                "original": 895,
                "effective": 895
            },
            "mmlu_high_school_european_history": {
                "original": 165,
                "effective": 165
            },
            "mmlu_logical_fallacies": {
                "original": 163,
                "effective": 163
            },
            "mmlu_prehistory": {
                "original": 324,
                "effective": 324
            },
            "mmlu_world_religions": {
                "original": 171,
                "effective": 171
            },
            "mmlu_moral_disputes": {
                "original": 346,
                "effective": 346
            },
            "mmlu_philosophy": {
                "original": 311,
                "effective": 311
            },
            "mmlu_high_school_us_history": {
                "original": 204,
                "effective": 204
            },
            "mmlu_formal_logic": {
                "original": 126,
                "effective": 126
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=Qwen/Qwen1.5-7B",
            "model_num_parameters": 7721324544,
            "model_dtype": "torch.bfloat16",
            "model_revision": "main",
            "model_sha": "831096e3a59a0789a541415da25ef195ceb802fe",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721416516.6273572,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3400.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "151643"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "151643"
        ],
        "tokenizer_bos_token": [
            null,
            "None"
        ],
        "eot_token_id": 151643,
        "max_length": 32768
    },
    "Qwen1.5-7B_tinyMMLU": {
        "results": {
            "tinyMMLU": {
                "alias": "tinyMMLU",
                "acc_norm,none": 0.5926792187235694,
                "acc_norm_stderr,none": "N/A"
            }
        },
        "group_subtasks": {
            "tinyMMLU": []
        },
        "configs": {
            "tinyMMLU": {
                "task": "tinyMMLU",
                "dataset_path": "tinyBenchmarks/tinyMMLU",
                "dataset_name": "all",
                "test_split": "test",
                "fewshot_split": "dev",
                "doc_to_text": "{{input_formatted}}",
                "doc_to_target": "answer",
                "doc_to_choice": [
                    "A",
                    "B",
                    "C",
                    "D"
                ],
                "description": "",
                "target_delimiter": " ",
                "fewshot_delimiter": "\n\n",
                "fewshot_config": {
                    "sampler": "first_n"
                },
                "num_fewshot": 0,
                "metric_list": [
                    {
                        "metric": "acc_norm",
                        "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
                        "higher_is_better": true
                    }
                ],
                "output_type": "multiple_choice",
                "repeats": 1,
                "should_decontaminate": false,
                "metadata": {
                    "version": 0.0
                }
            }
        },
        "versions": {
            "tinyMMLU": 0.0
        },
        "n-shot": {
            "tinyMMLU": 0
        },
        "higher_is_better": {
            "tinyMMLU": {
                "acc_norm": true
            }
        },
        "n-samples": {
            "tinyMMLU": {
                "original": 100,
                "effective": 100
            }
        },
        "config": {
            "model": "hf",
            "model_args": "pretrained=Qwen/Qwen1.5-7B",
            "model_num_parameters": 7721324544,
            "model_dtype": "torch.bfloat16",
            "model_revision": "main",
            "model_sha": "831096e3a59a0789a541415da25ef195ceb802fe",
            "batch_size": "1",
            "batch_sizes": [],
            "device": "cuda:0",
            "use_cache": null,
            "limit": null,
            "bootstrap_iters": 100000,
            "gen_kwargs": null,
            "random_seed": 0,
            "numpy_seed": 1234,
            "torch_seed": 1234,
            "fewshot_seed": 1234
        },
        "git_hash": "d0d36d2",
        "date": 1721395569.4911299,
        "pretty_env_info": "PyTorch version: 2.3.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Silver 4309Y CPU @ 2.80GHz\nStepping:                           6\nCPU MHz:                            3600.000\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nVirtualization:                     VT-x\nL1d cache:                          768 KiB\nL1i cache:                          512 KiB\nL2 cache:                           20 MiB\nL3 cache:                           24 MiB\nNUMA node0 CPU(s):                  0-7,16-23\nNUMA node1 CPU(s):                  8-15,24-31\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[pip3] triton==2.3.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.1                    pypi_0    pypi\n[conda] triton                    2.3.1                    pypi_0    pypi",
        "transformers_version": "4.42.3",
        "upper_git_hash": null,
        "tokenizer_pad_token": [
            "<|endoftext|>",
            "151643"
        ],
        "tokenizer_eos_token": [
            "<|endoftext|>",
            "151643"
        ],
        "tokenizer_bos_token": [
            null,
            "None"
        ],
        "eot_token_id": 151643,
        "max_length": 32768
    }
}